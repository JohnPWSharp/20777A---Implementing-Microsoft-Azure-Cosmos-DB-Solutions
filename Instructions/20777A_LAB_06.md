# Module 6: Implementing Server-Side Operations

- [Module 6: Implementing Server-Side Operations](#module-6-implementing-server-side-operations)
  - [Lab: Querying and analyzing big data with Cosmos DB](#lab-querying-and-analyzing-big-data-with-cosmos-db)
    - [Lab Scenario](#lab-scenario)
    - [Objectives](#objectives)
    - [Lab Setup](#lab-setup)
  - [Exercise 1: Extending product search capabilities](#exercise-1-extending-product-search-capabilities)
    - [Exercise 1 Scenario](#exercise-1-scenario)
    - [Prepare the Environment](#prepare-the-environment)
    - [Task 1: Create an Azure Search Service for the Cosmos DB database](#task-1-create-an-azure-search-service-for-the-cosmos-db-database)
    - [Task 2: Index the Data in the Cosmos DB Database](#task-2-index-the-data-in-the-cosmos-db-database)
    - [Task 3: Update the Adventure-Works app to use Azure Search](#task-3-update-the-adventure-works-app-to-use-azure-search)
    - [Task 4: Use the Suggester in the Search Field](#task-4-use-the-suggester-in-the-search-field)
  - [Exercise 2: Generating Month-end Order Summaries](#exercise-2-generating-month-end-order-summaries)
    - [Exercise 2 Scenario](#exercise-2-scenario)
    - [Task 1: Create an HDInsight Cluster for Running Spark](#task-1-create-an-hdinsight-cluster-for-running-spark)
    - [Task 2: Upload Sample Orders to the Cosmos DB Database](#task-2-upload-sample-orders-to-the-cosmos-db-database)
    - [Task 3: Install and Configure PuTTY](#task-3-install-and-configure-putty)
    - [Task 4: Perform Interactive Queries Against Cosmos DB Using Spark](#task-4-perform-interactive-queries-against-cosmos-db-using-spark)
    - [Task 5: Perform Batch Operations by Using a Jupyter Notebook](#task-5-perform-batch-operations-by-using-a-jupyter-notebook)
  - [Exercise 3: Visualizing Sales Data](#exercise-3-visualizing-sales-data)
    - [Exercise 3 Scenario](#exercise-3-scenario)
    - [Task 1: Create an Azure Databricks Service and Cluster](#task-1-create-an-azure-databricks-service-and-cluster)
    - [Task 2: Create a Library and PyDocumentDB Notebook](#task-2-create-a-library-and-pydocumentdb-notebook)
    - [Task 3: Write Python Code to Retrieve and Display the Data](#task-3-write-python-code-to-retrieve-and-display-the-data)
    - [Task 4: Create a Dashboard](#task-4-create-a-dashboard)
    - [Task 5: Cleanup the lab environment](#task-5-cleanup-the-lab-environment)

## Lab: Querying and analyzing big data with Cosmos DB

### Lab Scenario

Your company, Adventure Works, has a functioning online store. However, the customer support team has raised an issue that customers are complaining that they are unable to find the products with the current search. The head of IT has proposed making use of Azure Search to provide smart searching and auto suggestions to customers. They would also like to receive a new month end report to see if the search changes have improve sales.

### Objectives

At the end of this lab, you should be able to:

1. Add a search capability to an existing web app.
2. Use HDInsight and Spark to produce a monthly report.

### Lab Setup

-  **Estimated time**: 60 minutes
-  **Virtual machine**: 20777A-LON-DEV
-  **User name**: LON-DEV\\Administrator
-  **Password**: Pa55w.rd

## Exercise 1: Extending product search capabilities

### Exercise 1 Scenario

Customers can currently search for products by category, subcategory, name, model, or product id. You have been tasked with extending the search capabilities to include fields such as the product description. Additionally, you want to optimize the way in which the data in the Cosmos DB database is indexed, to reduce the data maintenance overhead in the database. You decide to use Azure Search to index the data and provide lookup services.

The main tasks for this exercise are as follows:

1. Prepare the environment.
2. Create an Azure Search service.
3. Index data in a Cosmos DB database.
4. Add a new search feature to the Adventure Works web applicaiton.

### Prepare the Environment

1. Ensure that the **MT17B-WS2016-NAT** and **20777A-LON-DEV** virtual machines are running, and then log on to **20777A-LON-DEV** as **LON-DEV\\Administrator** with the password **Pa55w.rd**.
2. In File Explorer, navgiate to **E:\\Labfiles\\Lab06\\Starter**, right-click **full-cosmos-setup.ps1.txt**, and then click **Edit**.
3. In Notepad, on line 2, edit **20777a-mod6-sql-\<*your initials and day*\>** and change the **\<*your initials and day*\>** to your initials and a number between 1 and 31, for example **20777a-mod6-sql-pjs14**.
4. On line 3, edit **20777a-mod6-\<*your initials and day*\>** and change the **\<*your initials and day*\>** to your initials and a number between 1 and 31, for example **20777a-mod6-pjs14**.
5. On line 4, edit **20777blobmod6\<*your initials and day*\>** and change the **\<*your initials and day*\>** to your initials and a number between 1 and 31, for example, **20777blobmod6pjs14**.
6. On line 5, set the **resourceGroupLocation** variable to the name of your nearest location.
7. On the **File** menu, click **Save**, and then close Notepad.
8. In File Explorer, right-click **Setup.cmd**, and then click **Run as administrator**.
9. If any **Security warning** messages appear, type **R**, and then press Enter.
10. When prompted enter your Azure credentials.
11. When the script completes, press any key to close the command window.

### Task 1: Create an Azure Search Service for the Cosmos DB database

1. In the Azure portal, in the left panel click **+ Create a Resource**.
2. On the **New** blade, in the **Search** box, type **Azure Search** then press Enter. Click **Azure Search**, then click **Create**.
3. On the **Azure Search** blade, in the **URL** box, type **20777a-search-&lt;your name&gt;-&lt;the day&gt;** for example, **20777a-search-john-31**.
4. Under **Resource Group**, select **Use existing**, and then select **20777Mod06**.
5. In the **Location** box, select the same location as your Cosmos DB account, or the closest physical location if the location you selected for your Cosmos DB account is not available.
6. Click **Pricing tier**.
7. On the **Choose your pricing tier** blade, click **Free**, and then click **Select**.
8. Click **Create**, and wait for the search account to be created before proceeding.
9. Make a note of the **Manage query keys**, and **&lt;empty&gt;** values in the **Keys** settings of the Search Service.

### Task 2: Index the Data in the Cosmos DB Database

1. In the Azure portal, go to your Cosmos DB account.
2. Click **Data Explorer**, select the **Adventure-Works** database, select the **Data** collection, and then click **Scale & Settings**.
3. Modify the **Indexing Policy** to include the excluded paths shown below, and then click **Save**. This policy prevents the **description**, **documentation**, and **images** fields from being indexed. The **description** field is free text and can be quite large; you will configure Azure Search to index this field more efficiently. The **documentation** and **images** fields are references to items in blob storage, and indexing these fields is not worthwhile.

    ```JSON
    {
        "indexingMode": "consistent",
        "automatic": true,
        "includedPaths": [
            {
                "path": "/*",
                "indexes": [
                    {
                        "kind": "Range",
                        "dataType": "Number",
                        "precision": -1
                    },
                    {
                        "kind": "Range",
                        "dataType": "String",
                        "precision": -1
                    },
                    {
                        "kind": "Spatial",
                        "dataType": "Point"
                    }
                ]
            }
        ],
        "excludedPaths": [
            {
                "path": "/description/*"
            },
            {
                "path": "/documentation/*"
            },
            {
                "path": "/images/*"
            }
        ]
    }
    ```
4. Return to your Search Service account in the Azure portal.
5. Click **Import Data**.
6. In the **Import data** blade, click **Connect to your data**, and then click **Cosmos DB**.
7. In the **New data source** blade, in the **Name** box, type **productdata**.
8. Under **Cosmos DB acount**, click **Select an account**, and select your Cosms DB account.
9. In the **Database** field, select **Adventure-Works**, in the **Collection** field, select **Data**, and then click **OK**.
10. In the **Import Data** blade, under **Index**, click **Customize target index**.
11. Delete the **ttl** field from the index.
12. Mark all remaining fields as **Retrievable**.
13. Mark the following fields as **Filterable**:
    - **id**
    - **partitionkey**
    - **productcategory**
    - **productname**
    - **productnumber**
    - **model**
    - **description**
    - **doctype**
14. Mark the following fields as **Searchable**:
    - **productcategory**
    - **productname**
    - **productnumber**
    - **model**
    - **description**
15. Select **Suggester**, set the **suggester name** to **suggest**, and set the **Search mode** to **analyzingInfixMatching**.
16. Mark the following fields to use the suggester, and then click **OK**:
    - **productname**
    - **model**
    - **description**
17. In the **Create an Indexer** blade, set the **Name** to **productsindexer**, change the **Schedule** to **Hourly**, and then click **OK**.
18. On the **Import data** blade, click **OK**.
19. On the search service blade, under **Usage**, click the **Indexers** box.
20. On the **Indexers** blade, click the **productsindexer** index.
21. On the **productsindexer** blade, click **Run**, and then click **Yes**.
22. Wait for a minute, close the **productsindexer** blade, and then in the **Indexers** blade, click the **productsindexer** index again.
23. On the **productsindexer** blade, verify that indesing was completed successfully.
24. Close the **productsindexer** blade.
25. Close the **Indexers** blade.
26. On the search service blade, click **Search explorer**.
27. On the **Search explorer** blade, click **Search**. Verify that the search returns a list of documents.
28. In the **Query string** box, type **search=bike**, and then click **Search**. Verify that a list of documents appear that include the text **bike** in one of the searchable fields.
29. In the **Query string** box, type **search=natural**, and then click **Search**. This time only two documents should be retrieved. Both documents include the word **natural** in the **description** field.

### Task 3: Update the Adventure-Works app to use Azure Search

1. Using Visual Studio 2017, open the **Adventure-Works** solution in the **E:\Labfiles\Lab06\Starter\Exercise 1\Adventure-Works** folder.
2. In Solution Explorer, open the **Repository.cs** file. The **Repository** implements the functionality required by the app to retrieve and maintain data in the Cosmos DB database. You will extend this class to incorporate Azure Search capabilities.
3. Find the comment **TODO: Add namespaces containing Search functionality**, and add the following using directives:

    ```CSharp
    using Microsoft.Azure.Search;
    using Microsoft.Azure.Search.Models;
    ```

4. In the **Repository** class, after the comment **TODO: Define fields to support connecting to the Azure Search Service**, add the following statements. This code defines the variables that the repository will use to connect to the search service. The **SearchIndexClient** type provides the methods that an app can use to send search requests and retrieve the results.  

    ```CSharp
    private static string searchService;
    private static string queryKey;
    private static string searchIndex;
    private static string suggesterName;
    private static SearchIndexClient searchClient;
    ```

5. In the **Initialize** method, add the following statements after the comment **TODO: Initialize the connection to the Azure Search Service**. These statements read the parameters required to connect to the Azure Search Service from the web.config file. You will provide values for these parameters when you configure the application later:

    ```CSharp
    searchService = ConfigurationManager.AppSettings["SearchService"];
    queryKey = ConfigurationManager.AppSettings["SearchServiceQueryKey"];
    searchIndex = ConfigurationManager.AppSettings["SearchIndex"];
    suggesterName = ConfigurationManager.AppSettings["Suggester"];
    searchClient = new SearchIndexClient(searchService, searchIndex, new SearchCredentials(queryKey));
    ```

6. Find the **SearchForItemsAsync** method shown below. You will complete this method to use the Azure Search Service to find all documents in the Cosmos DB database that match a specified search term passed in as the **SearchString** parameter. The **advanced** parameter indicates whether the string in the **SearchString** parameter should be treated as a literal value, or it contains an Azure Search expression:

    ```CSharp
    // TODO: Use Azure Search to find items that match the specified search string
    public static async Task<IEnumerable<T>> SearchForItemsAsync(string searchString, bool advanced)
    {
        // TODO: If this is not an advanced query, enclose the search string in quotes to ensure that Azure Search treats it as a literal rather than a query expression

        // TODO:  Specify that the search should only return the id and partitionkey fields of the document from the index

        // TODO: Find the ids of all documents that match the search string

        // TODO: Iterate through the results, and construct a list of matching documents from the Cosmos DB database
        List<T> docs = new List<T>();
        foreach (var result in searchResults.Results)
        {
            // TODO: Construct the document URI for the document

            // TODO: Extract the partition key from the index

            // TODO: Fetch the document and add it to the list

        }

        // TODO: Return the list of documents identified by the search

    }
    ```

7. After the comment **TODO: If this is not an advanced query, enclose the search string in quotes to ensure that Azure Search treats it as a literal rather than a query expression**, add the following block of code. If the **advanced** parameter is false, these statements add quotes to search string to ensure that it is handled as a literal value by the Azure Search Service:

    ```CSharp
    if (!advanced)
    {
        searchString = $"\"{searchString}\"";
    }
    ```

8. After the comment **TODO:  Specify that the search should only return the id and partitionkey fields of the document from the index**, add the following statements. The purpose of the Azure Search Service index is to quickly locate the id of matching documents in the Cosmos DB database. When you retrieve these documents from the database you will also require the partition key:

    ```CSharp
    var searchParams = new SearchParameters
    {
        Select = new [] { "id", "partitionkey" }
    };
    ```

9. After the comment **TODO: Find the ids of all documents that match the search string**, add the following code that sends the search request to the Azure Search Service.

    ```CSharp
    var searchResults = await searchClient.Documents.SearchAsync<dynamic>(searchString, searchParams);
    ```

10. In the **foreach** loop that iterates through the results of the search request, after the comment **TODO: Construct the document URI for the document**, add the following statements. This code uses the id returned by the Azure Search Service to build the URI of the corresponding document in the Cosmos DB database.

    ```CSharp
    string docID = result.Document.id;
    Uri docUri = UriFactory.CreateDocumentUri(database, collection, docID);
    ```

11. After the comment **TODO: Extract the partition key from the index**, add the following statements. The **RequestOptions** object will be passed to a Cosmos DB request that fetches the document:

    ```CSharp
    string partitionkey = result.Document.partitionkey;
    var options = new RequestOptions
    {
        PartitionKey = new PartitionKey(partitionkey)
    };
    ```

12. After the comment **TODO: Fetch the document and add it to the list**, add the following statements. Racall that the **ReadDocumentAsync** method provides the fastest way to access data in a Cosmos DB database:

    ```CSharp
    var response = await client.ReadDocumentAsync<T>(docUri, options);
    docs.Add(response.Document);
    ```

13. After the comment **TODO: Return the list of documents identified by the search**, replace that code that throws the **NotImplementedException** with the following statement:

    ```CSharp
    return docs;
    ```

14. In Solution Explorer, expand the **Models** folder, and then double-click **ViewModel.cs**.
15. In the **ProductViewModel** class, after the comment **TODO: Add the "AdvancedSearch" field to the view model, to record whther the user has selected a simple text or expression-based search**, add the following property:

    ```CSharp
    public bool AdvancedSearch { get; set; }
    ```

16. In Solution Explorer, expand the **Controllers** folder, and then double-click the **ProductsController.cs** file.
17. Find the **SearchProductsAsync** method. You will complete this method to search for products using the repository.
18. After the comment **TODO: Use the repository to find products that match the specified search string in the Product View Model**, add the following statement:

    ```CSharp
    IEnumerable<Product> products = await Repository<Product>.SearchForItemsAsync(productViewModel.SearchString, productViewModel.AdvancedSearch);
    ```

19. After the comment **TODO: Construct a new view model containing the results and display it**, replace the statement that throws the **NotImplementException** with the following code. This statement constructs a new view model containing the results of the search, and sends the view model back to the view for display:

    ```CSharp
    return View("FindProducts", new ProductViewModel
    {
        Products = products,
        SelectableCategories = Session["selectableCategories"] as ProductCategoryViewModel ?? await InitializeCategoriesAsync(),
        SelectableSubcategories = Session["selectableSubcategories"] as ProductSubcategoryViewModel ?? await InitializeSubCategoriesAsync()
    });
    ```

20. In Solution Explorer, expand the **Views** folder, expand the **Products** folder, and then double-click the **FindProducts.cshtml** file.
21. Find the comment **&lt;!-- TODO: Adjust the form to enable the user to specify a simple or advanced search --&gt;**. Note that this form calls the **SearchProducts** action (which runs the **SearchProductsAsync** method) in the **ProductsController**.
22. Replace the **&Html.TextBoxFor** statement after this comment with the following markup. This code adds the search string entered by the user on the form, together with a boolean indicating whether the user selected the **Advanced** check box, to the view model which is passed to the **SearchProductsAsync** method:

    ```HTML
    <table class="table">
        <tr>
            <td>
                @Html.TextBoxFor(t => t.SearchString, new { @class = "form-control" })
            </td>
            <td>
                @Html.Label("AdvancedSearch", "Advanced?", new { @class = "form-control" })
            </td>
            <td>
                @Html.CheckBoxFor(t => t.AdvancedSearch, new { @class = "form-control" })
            </td>
        </tr>
    </table>
    ```

23. Edit the Web.config file and specify values for the following settings:
    - **EndpointUrl**. The URL of your Cosmos DB account
    - **PrimaryKey**. The primary key for your Cosmos DB account
    - **SearchService**. The name of your Azure Search Service.
    - **SearchServiceQueryKey**. The query key for your Azure Search Service.
24. Build and run the app.
25. In the **Search for Products** section of the form, enter **natural**, and then click **Search**. Verify that the search returns two products (Mountain Bike Socks, M and L). On either row, click **Details**. The **Description** contains the following text:

    ```Text
    Combination of natural and synthetic fibers stays dry and provides just the right cushioning.
    ```

26. Go back to the **Adventure-Works Product Catalog** page and search for **Bib-Shorts**. The results should list three products (Men's Bib-Shorts, S, M and L).
27. Select the **Advanced?** option and then click **Search** again. This time the search returns more products. The search string has been interpreeted as a query expression, and now finds all products that contain the text **Bib** and/or **Shorts**
28. Experiment with other searches. When you have finished, close the app and return to Visual Studio.

### Task 4: Use the Suggester in the Search Field

1. In Visual Studio, return to the **Repository.cs** file and find the **GetSuggestions** method. THis method takes a string parameter, and will use the Azure Search Service Suggester that you created earlier to suggest matching terms from the search index.
2. After the comment **TODO: Specify the Suggestion parameters: enable fuzzy searching, fetch the top 10 results, and add highlighting**, add the following code:

    ```CSharp
    var parameters = new SuggestParameters
    {
        UseFuzzyMatching = true,
        Top = 10,
        HighlightPreTag = "<b>",
        HighlightPostTag = "</b>"
    };

    ```

3. After the comment **TODO: Retrieve suggestions from the Azure Search service**, add the following statement:

    ```CSharp
    DocumentSuggestResult result = await searchClient.Documents.SuggestAsync(term, suggesterName, parameters);
    ```

4. After the comment **TODO: Return the suggestions as a list of strings**, replace the code that throws the **NotImplementedException** with the following statement. This statement extracts the **Text** field from each of the results and converts these items into a list:

    ```CSharp
    return result.Results.Select(x => x.Text).ToList();
    ```

5. Edit the **ProductControler.cs** file and find the **SuggestAsync** method shown below. This method responds to the **Suggest** HTTP GET request. The parameter is a string on which to base suggestions:

    ```CSharp
    // TODO: Method called by the autocomplete feature of the form to use the Suggester in Azure Search to "suggest" search strings
    [ActionName("Suggest")]
    [HttpGet]
    public async Task<ActionResult> SuggestAsync(string term)
    {
        // TODO: Call the GetSuggestions method in repository to find suggestions that match the term provided

        // TODO: Return the list of suggestions
        throw new NotImplementedException();
    }
    ```

6. After the comment **TODO: Call the GetSuggestions method in repository to find suggestions that match the term provided**, add the following statement:

    ```CSharp
    var suggestions = await Repository<Product>.GetSuggestions(term);
    ```

7. After the comment **TODO: Return the list of suggestions**, replace the code that throws the **NotImplementedException** with the following statement:

    ```CSharp
    return new JsonResult
    {
        JsonRequestBehavior = JsonRequestBehavior.AllowGet,
        Data = suggestions
    };
    ```

8. Return to the **FindProducts.cshtml** view and find the comment **&lt;!-- TODO: Add jquery-ui stylesheet and function to add autocomplete to the SearchString text field--&gt;** near the top of the file. You will use the **autocomplete** feature of the jquery-ui package to send the **Suggest** request to the controller as te user enters data into the **Search** field.
9. Add the following markup to the view. This code adds a reference to the style-sheet that is used by the jquery-ui package:

    ```HTML
    <link rel="stylesheet" href="//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
    ```

10. Inside the **&lt;script&gt;** tag, after the comment **TODO: Replace anything that looks like HTML markup from the SearchString field after performing autocomplete**, add the following JavaScript function:

    ```JavaScript
    var updateTextbox = function (event, ui) {
        var result = ui.item.value.replace(/<\/?[^>]+(>|$)/g, "");
        $("#SearchString").val(result);
        return false;
    };
    ```

11. After the comment **TODO: Trigger autocomplete on the SearchString field to invoke the Suggest method on the form. The data typed so far is provided as an input parameter to the method**, add the following code. The call to the jquery-ui **autocomplete** function invokes the **Suggest** action once the user has entered three or more characters. The JSON data returned by the **Suggest** is displayed as a drop-down listbox by the **autocomplete** function. When the user selects an item from the list, any HTML tags (such as those generated by Azure Suggester highlighting) are stripped out using the **updateTextbox** function as the item is written to the **SearchString** field:

    ```JsvaScript
    $("#SearchString").autocomplete({
        html: true,
        source: "/Products/Suggest",
        minLength: 3,
        select: updateTextbox,
        focus: updateTextbox
    }).data("ui-autocomplete")._renderItem = function (ul, item) {
        return $("<li></li>")
            .data("item.autocomplete", item)
            .append("<a>" + item.label + "</a>")
            .appendTo(ul);
        };
    ```

12. Build and run the app.
13. In the **Search** box, type **nat**. A list of suggestions should appear, with the matching text highlighted in bold. Note that similar items are included that are not necessarily an exact match for the text; this is because the repository uses fuzzy matching with the suggester.
14. Change the text to **natu**, and the list should be refined to two entries.
15. Click one of the entries and then click **Search**. The result should be two products.
16. Click **Details**, and you should see that both products have identical descriptions containing the word **natural**.
17. Clear the **Search** box, and then enter **bib**. Select **Men's Bib-Shorts, M** and then click **Search**. This time only a single product should be displayed.

    > **Note:** If you select **Advanced** and click **Search** again, you will get more results. This is because the text in the search box is now interpreted as a query expression, and some of the characters have special meanings.
18. Experiment with other suggestions and searches. Close the app when you have finished.

## Exercise 2: Generating Month-end Order Summaries

### Exercise 2 Scenario

As part of the month-end processing, you need to generate summaries that indicate how many orders were placed, the total value of those orders, and which were the most popular products for that month.

### Task 1: Create an HDInsight Cluster for Running Spark

1. In the Azure Portal, in the left pane, click **+ Create a resource**.
2. Click **Analytics**, and then click **HDInsight**.
3. On the **HDInsight** blade, click **Custom (size, settings, apps)**.
4. On the **Basics** blade, type the following details, and then click **Cluster type**:
    - Cluster name: **hdi-&lt;your name&gt;&lt;date&gt;**
    - Subscription: **your subscription**
5. On the **Cluster configuration** blade, enter the following details, and then click **Select**:
    - Cluster type: **Spark**
    - Version: **Spark 2.3.0 (HDI 3.6)**
6. On the **Basics** blade, enter the following details:
    - Cluster login username: **sparkadmin**
    - Cluster login password: **Pa55w.rdPa55w.rd**
    - Secure Shell (SSH) username: **sadmin**
    - Use same password as cluster login: **selected**
    - Resource group (use existing): **20777Mod06**
    - Location: Select your region
7. On the **Basics** blade, click **Next**.
8. On the **Storage** blade, under **Select a Storage account**, click **Create new**.
9. In the **Create a new Storage account** box, type **&lt;your name&gt;&lt;date&gt;sa**.
10. In the **Default container** box, replace the suggested name with the name of your cluster.
11. Leave the other settings at their defaults, and then click **Next**
12. On the **Applications (optional)** blade, click **Next**.
13. On the **Cluster size** blade, in the **Number of Worker nodes** box, type 2.
14. Click **Worker node size**.
15. On the **Choose your node size blade**, click **D12 v2 Optimized**, and then click **Select**.
16. Click **Head node size**.
17. On the **Choose your node size** blade, click **View all**, click **D12 v2 Optimized**, and then click **Select**.
18. On the **Cluster size** blade, click **Next**.
19. On the **Advanced settings** blade, click **Next**.
20. On the **Cluster summar**y blade, click **Create**.
21. Wait for the cluster to be provisioned and the status to show as **Running**. This is likely to take at least 10 minutes. You can continue with the next task while cluster is being built.

### Task 2: Upload Sample Orders to the Cosmos DB Database

1. Using Visual Studio, open **GenerateOrderData** solution in the **E:\\Labfiles\\Lab06\\Data\\GenerateOrderData** folder.
2. In Solution Explorer, open the **app.config** file.
3. Specify the following configuration settings in the **appSettings** section:
    - **EnpointUrl**: The URL of your Cosmos DB account.
    - **PrimaryKey**: The primary access key for the Cosmos DB account.
    - **Database**: Adventure-Works
    - **Collection**: Data
    - **NumOrders**: 5000. This is the number of orders that will be created.
4. Build and run the application. The orders will be generated quckly, but might take a minute or two to be uploaded to the database.
5. In the Azure portal, go to your Cosmos DB account.
6. Select **Data Explorer**, select the **Adventure-Works** database, and then select the **Data** collection.
7. Click **New SQL Query**, and enter the following query:

    ```SQL
    SELECT * FROM c WHERE c.doctype = "ShoppingCartOrder"
    ```

8. Click **Execute Query**, and examine the first few orders that have been created.
9. Change the query as follows:

    ```SQL
    SELECT VALUE COUNT(1) FROM c WHERE c.doctype = "ShoppingCartOrder"
    ```

10. Click **Execute Query**, and verify that the database contains 5000 orders.

### Task 3: Install and Configure PuTTY

1. Using Internet Explorer, browse to **https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html**.
2. Download the 64-bit version of putty.exe and save it as **E:\putty\putty.exe**.
3. Download 64-bit version of pscp.exe, and save it as **E:\putty\pscp.exe**.
4. Return to the Azure Portal and verify that the HDInsight cluster has been created. If not, wait for prvisioning to complete.
5. Click **All Resources**.
6. On the **All Resources** blade, click the HDInsight cluster.
7. On the cluster blade, click **SSH + Cluster Login**.
8. On the **SSH + Cluster login** blade, in the **Hostname** box, select the name of your cluster (it should be the only entry), then click the **Click to copy** button (two document icons on a blue background).
9. In File Explorer, navigate to **E:\putty**, and then double-click **putty.exe**.
10. In the **PuTTY Configuration** window, right-click the **Host Name (or IP Address)** box, and paste the hostname stored on the clipboard. Edit the pasted value to remove **ssh sadmin@** at the beginning of the host name.
11. In the **Saved Sessions** box, type **HDInsight**, click **Save**, and then click **Open**.
12. In the **PuTTY Security Alert** window, click **Yes**.
13. In the PuTTY window, at the **login as** prompt, login as **sadmin** with password **Pa55w.rdPa55w.rd**. If login is successful, you will be presented with a prompt that starts **sadmin@hn0-**.

### Task 4: Perform Interactive Queries Against Cosmos DB Using Spark

1. At the prompt in PuTTY, type the following command and wait for the Spark shell to start running:

    ```Bash
    spark-shell --master yarn --packages com.microsoft.azure:azure-cosmosdb-spark_2.3.0_2.11:1.2.0
    ```
2. When the **scala>** appears, enter the following commands:

    ```Scala
    import com.microsoft.azure.cosmosdb.spark.schema._
    import com.microsoft.azure.cosmosdb.spark._
    import com.microsoft.azure.cosmosdb.spark.config._
    ```

3. Enter the following command that creats a configuration object that you will use to retrieve the orders held in the Cosmos DB database. Replace **\~URL\~** with the URL of yor Cosmos DB account, and replace **\~KEY\~** with the primary access key for the account:

    ```Scala
    val databaseConfig = Config(Map(
        "Endpoint" -> "~URL~",
        "Masterkey" -> "~KEY~",
        "Database" -> "Adventure-Works",
        "Collection" -> "Data",
        "query_custom" -> "SELECT * FROM c WHERE c.doctype = 'ShoppingCartOrder'"
    ))
    ```

4. Run the following command. This command executes the query and creats a temporary Hive view called **orders** that references the results (the data in the view is not persisted):

    >**Note:** You will receive a warning specifying that JNI has not been loaded. You can ignore this warning.

    ```Scala
    val collection = spark.sqlContext.read.cosmosDB(databaseConfig)
    collection.createOrReplaceTempView("orders")
    ```

5. Run the following query, and verify that it fins 5000 rows. Note that the query runs as a Spark job:

    ```Scala
    val data = spark.sql("SELECT * FROM orders")
    data.count()
    ```

6. Run the following command. This command shows the schema generated for the orders view, based on the data returned by the query in the configuration object.

    ```Scala
    data.printSchema
    ```

7. Notice that the **orderitems** field is structured as an array in the view:

    ```Text
    root
    |-- _attachments: string (nullable = true)
    |-- _etag: string (nullable = true)
    |-- _rid: string (nullable = true)
    |-- _self: string (nullable = true)
    |-- _ts: integer (nullable = true)
    |-- customerdiscountrate: integer (nullable = true)
    |-- dateplaced: long (nullable = true)
    |-- doctype: string (nullable = true)
    |-- id: string (nullable = true)
    |-- isshoppingcartororder: string (nullable = true)
    |-- itemscost: double (nullable = true)
    |-- lastupdated: long (nullable = true)
    |-- numberofitems: integer (nullable = true)
    |-- orderitems: array (nullable = true)
    |    |-- element: struct (containsNull = false)
    |    |    |-- backorderreference: string (nullable = true)
    |    |    |-- lineitemtotalcost: double (nullable = true)
    |    |    |-- numberincartorordered: integer (nullable = true)
    |    |    |-- numberonbackorder: integer (nullable = true)
    |    |    |-- productid: string (nullable = true)
    |    |    |-- productname: string (nullable = true)
    |    |    |-- productnumber: string (nullable = true)
    |    |    |-- subcategory: string (nullable = true)
    |    |    |-- unitcost: double (nullable = true)
    |-- orderstatus: string (nullable = true)
    |-- partitionkey: string (nullable = true)
    ...
    |-- totalcost: double (nullable = true)
    |-- ttl: integer (nullable = true)

    ```

8. Perform the following query. This query returns the name of the first product in each order together with the number ordered (the output is truncated to display only the first 20 orders):

    ```Scala
    spark.sql("SELECT orderitems[0].productname, orderitems[0].numberincartorordered FROM orders").show()
    ```

9. Run the following query. This query reports the total value of orders that are in progress, that have been delivered, and that have been cancelled:

    ```Scala
    spark.sql("SELECT SUM(totalcost), orderstatus FROM orders GROUP BY orderstatus").show()
    ```

10. Experiment by running other queries against the **orders** view.
11. When you have finished, type **:quit**, press Enter, and then press CTRL+C to close the spark shell.

### Task 5: Perform Batch Operations by Using a Jupyter Notebook

1. Open a command prompt window and go to the **E:\Labfiles\Lab06\starter\Exercise 2** folder.
2. Run the following command. This command uses the pscp utility to upload the jar file containing the Cosmos DB connector for Spark to the first head node of your HDInsight cluster. Replace **&lt;cluster-name&gt;** with **hdi-&lt;your name&gt;&lt;date&gt;**. Enter **Pa55w.rdPa55w.rd** when prompted for a password:

    ```Script
    e:\putty\pscp azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar sadmin@<cluster-name>-ssh.azurehdinsight.net:azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar
    ```

3. Open the HDInsight PuTTY session, and connect using **sadmin** and **Pa55w.rdPa55w.rd**.
4. In the PuTTY window, run the following command. Verify that the file **azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar** is listed:

    ```Bash
    ls
    ```

5. Run the following command to copy the Cosmos DB connector to the folder required by Spark applications:

    ```Bash
    sudo cp *.jar /usr/hdp/current/spark2-client/jars/
    ```

6. Run the following commands to copy the Cosmos DB connector to the same folder on the remaining head node and worker node of the HDInsight cluster. At each prompt **Are you sure you want to continue connecting**, type **yes**, and enter the pasword **Pa55w.rdPa55w.rd**:

    ```Bash
    hn=`hostname` && hn1="${hn/hn0-/hn1-}"
    rcp azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar $hn1:azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar
    hn=`hostname` && wn0="${hn/hn0-/wn0-}"
    rcp azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar $wn0:azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar
    hn=`hostname` && wn1="${hn/hn0-/wn1-}"
    rcp azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar $wn1:azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar
    ssh $hn1
    sudo cp *.jar /usr/hdp/current/spark2-client/jars/
    exit
    ssh $wn0
    sudo cp *.jar /usr/hdp/current/spark2-client/jars/
    exit
    ssh $wn1
    sudo cp *.jar /usr/hdp/current/spark2-client/jars/
    exit
    ```

    >**Note:** If the **wn1** node shows an error, replace **wn1** with **wn2** and re-run the command.

7. Return to the Azure portal and go to your HDInsight cluster.
8. On the **HDInsight cluster** blade, in the **Quick Links** section, click **Cluster dashboard**, and then click **Ambari Home**.
9. Log in as **sparkadmin** with password **Pa55w.rdPa55w.rd** when prompted.
10. Restart the **Spark2** services.
11. In the Azure portal, on the **Cluster dashboards** blade, click **Jupyter Notebook**. Internet Explorer opens a new page connected to the Jupyter UI.
12. On the **Jupyter** page, click **New** and then click **Spark**. This action creates a Scala notebook that can perform Spark queries.
13. In the page heading, click **Untitled** and change the name of the notebook to **Monthly Analysis**.

    >**Note:** The complete code for the notebook is available in the file **E:\\Labfiles\\Lab06\\Solution\\Exercise 2\\Monthly+Analysis.txt**.

14. In the first cell of the notebook, add the following Scala code. This code creates a useful utility class that can convert between datetime values and ticks (as used by the documents in the CosmosDB database):

    ```Scala
    // Helper class for converting between dates and ticks

    import java.util.Calendar
    import java.util.Date

    object DateHelper {
        private final val TICKS_AT_EPOCH = 621355968000000000L
        private final val TICKS_PER_MILLISECOND = 10000

        def getUTCTicks(date: Date): Long = {
            val calendar = Calendar.getInstance
            calendar.setTime(date)

            return (calendar.getTimeInMillis() * TICKS_PER_MILLISECOND) + TICKS_AT_EPOCH
        }

        def getDate(ticks: Long): Date = {
            return new Date((ticks - TICKS_AT_EPOCH) / TICKS_PER_MILLISECOND)
        }
    }
    ```

15. On the **Insert** menu, click **Insert Cell Below**.
16. In the new cell, add the following code

    ```Scala
    // Get the time in ticks one month ago

    val calendar = Calendar.getInstance
    calendar.add(Calendar.MONTH, -1)
    val then = calendar.getTime
    val thenInTicks = DateHelper.getUTCTicks(then)
    ```

17. On the **Insert** menu, click **Insert Cell Below**.
18. In the new cell, add the following code. This code imports the libraries required by the Cosmos DB connector:

    ```Scala
    import com.microsoft.azure.cosmosdb.spark.schema._
    import com.microsoft.azure.cosmosdb.spark._
    import com.microsoft.azure.cosmosdb.spark.config._
    ```

19. On the **Insert** menu, click **Insert Cell Below**.
20. In the new cell, add the following code. This uses an %%sql magic to drop the orders table in the Hive database if it currently exists. This step is nessary as you will create a virtual table, named orders, that references the orders in the Data collection in the CosmosDB database:

    ```SQL
    %%sql
    -- Drop the orders table if it currently exists
    DROP TABLE IF EXISTS orders
    ```

21. On the **Insert** menu, click **Insert Cell Below**.
22. Add the following SQL magic to the new cell. Replace **\~URI\~** with the URI of your Cosmos DB acount, and replace **\~KEY\~** with the primary access key for the account. This code creates a virtual table that references the orders data in the Cosmos DB database directly rather than by creating a Hive view (the data is not copied, rather the query is performed everytime the table is accessed):

    ```SQL
    %%sql
    -- Rebuild the orders table as a reference to the Data collection in the Cosmos DB database
    -- Don't include the orderitems subdocuments - these are arrays and are better handled as a seperate external table

    CREATE TABLE orders using com.microsoft.azure.cosmosdb.spark options (
    endpoint "~URI~",
    masterkey "~KEY~",
    database "Adventure-Works",
    collection "Data",
    query_custom "SELECT c.id, c.numberofitems, c.itemscost, c.customerdiscountrate, c.totalcost, c.dateplaced, c.orderstatus, c.lastupdated FROM c WHERE c.doctype = 'ShoppingCartOrder'")
    ```

23. On the **Insert** menu, click **Insert Cell Below**.
24. In the new cell, add the following code. This drops the orderitems table in the Hive database. You will create another table that contains the order items from all of the orders:

    ```SQL
    %%sql
    -- Drop the orderitems table if it currently exists
    DROP TABLE IF EXISTS orderitems
    ```

25. On the **Insert** menu, click **Insert Cell Below**.
26. Add the following SQL magic to the new cell. As before, replace **\~URI\~** with the URI of your Cosmos DB acount, and replace **\~KEY\~** with the primary access key for the account. This code creates another virtual table that references the order items for each order in the Cosmos DB database. This data is held as subdocuments, and each subdocument contains an array of order items. The query that fetches the data for this table uses a CosmosDB JOIN clause to flatten this data out into a simple table which makes the data easier to handle using Spark SQL.:

    ```SQL
    %%sql
    -- Get the orderitems for orders placed in the last month. The JOIN clause flattens the arrays in the subdocuments into a tabular format

    CREATE TABLE orderitems using com.microsoft.azure.cosmosdb.spark options (
    endpoint "~URI~",
    masterkey "~KEY~",
    database "Adventure-Works",
    collection "Data",
    query_custom "SELECT o.id, o.dateplaced, o.orderstatus, i.productnumber, i.productname, i.numberincartorordered, i.lineitemtotalcost FROM o JOIN i IN o.orderitems WHERE o.doctype = 'ShoppingCartOrder'")
    ```

27. On the **Insert** menu, click **Insert Cell Below**.
28. Add the following Scala code to the new cell. This code creates a Spark dataframe that retrieves all documents from the CosmosDB collection (via the orders virtual table) for orders that have been placed in the last month:

    ```Scala
    // Find the status and cost of all orders placed in the last month
    // Note: Only retrieve the necessary columns (not "*"), to keep the dataframe lean

    val orders = spark.sqlContext.sql(s"SELECT orderstatus, totalcost FROM orders WHERE dateplaced > $thenInTicks")
    ```

29. On the **Insert** menu, click **Insert Cell Below**.
30. Add the following  Scala code to the new cell. This code creates a new dataframe that summarizes the data just fetched, to generate a count of the number of orders in the last month, grouped by order status:

    ```Scala
    // Find the total number of orders, grouped by order status ("In progress", "Delivered", "Cancelled")

    val numbersByStatus = orders.groupBy("orderstatus").count()
    numbersByStatus.show
    ```

31. On the **Insert** menu, click **Insert Cell Below**.
32. Add the following Scala code to the new cell. This code generates a dataframe that shows the total revenue grouped by order status. Using this dataframe you can quickly see how much revenue has been generated by delivered orders and orders currently in progress, as well as determine how much money was refunded due to cancelled orders:

    ```Scala
    // Find the sum value of the revenue for all orders, grouped by order status

    val revenuesByStatus = orders.groupBy("orderstatus").sum("totalcost")
    revenuesByStatus.show
    ```

33. On the **Insert** menu, click **Insert Cell Below**.
34. Add the following Scala code to the new cell. This code generates a dataframe that summarizes information about the items ordered in the last month, retrieved using the orderitems virtual table. You can use this dataframe to establish which were the most popular products, and which products generated the most revenue:

    ```Scala
    // Find the items ordered in the last month

    val orderitems = spark.sqlContext.sql(s"SELECT productnumber, productname, SUM(numberincartorordered) AS numordered, SUM(lineitemtotalcost) AS revenue FROM orderitems WHERE dateplaced > $thenInTicks AND orderstatus <> 'Cancelled' GROUP BY productnumber, productname")
    orderitems.show(5)
    ```

35. On the **Insert** menu, click **Insert Cell Below**.
36. Add the following Scala code to the new cell. This code sorts the order item summary records by number ordered. The result shows the products in decreasing order or popularity:

    ```Scala
    // Sort the data in descending order of volume (numordered)

    val orderedByVolume = orderitems.sort($"numordered".desc)
    orderedByVolume.show(5)
    ```

37. On the **Insert** menu, click **Insert Cell Below**.
38. Add the following Scala code to the new cell. This code sorts the order item summary records by revenue:

    ```Scala
    // Sort the data in descending order of revenue per product

    val orderedByRevenue = orderitems.sort($"revenue".desc)
    orderedByRevenue.show(5)
    ```

39. On the **Insert** menu, click **Insert Cell Below**.
40. Add the following Scala code to the new cell. This code configures a connection that you will use to write the summary data just generated back to the Cosmos DB database. Replace **\~URI\~** and **\~KEY\~** with the values for your Cosmos DB account:

    ```Scala
    // Write the summaries back to Cosmos DB with doctype "MonthlySummary"

    import org.apache.spark.sql._

    val writeConfigMap = Map(
        "Endpoint" -> "~URI~",
        "Masterkey" -> "~KEY~",
        "Database" -> "Adventure-Works",
        "Collection" -> "Data"
    )

    val writeConfig = Config(writeConfigMap)
    ```

41. On the **Insert** menu, click **Insert Cell Below**.
42. Add the following Scala code to the new cell. This code creates a text value containing the month and year. You will add this value to the summary data:

    ```Scala
    val month = calendar.get(Calendar.MONTH) + 1
    val year = calendar.get(Calendar.YEAR)
    val monthYear = s"$month/$year"
    ```

43. On the **Insert** menu, click **Insert Cell Below**.
44. Add the following Scala code to the new cell. This code augments the "orders by status" dataframe with three columns; doctype ("MonthlySummary"), month (the text value just created), and partitionKey ("summary"). It then uses the connection to the Cosmos DB database to write this data as a document back to the Cosmos DB database:

    ```Scala
    val numbers = numbersByStatus.withColumn("doctype", functions.lit("MonthlySummary")).withColumn("month", functions.lit(monthYear)).withColumn("partitionKey", functions.lit("summary"))
    numbers.write.mode(SaveMode.Append).cosmosDB(writeConfig)
    numbers.show(5)
    ```

45. On the **Insert** menu, click **Insert Cell Below**.
46. Add the following Scala code to the new cell. This code is similar to the previous cell except that it saves the data in the "revenues by status" dataframe:

    ```Scala
    val revenues = revenuesByStatus.withColumn("doctype", functions.lit("MonthlySummary")).withColumn("month", functions.lit(monthYear)).withColumn("partitionKey", functions.lit("summary"))
    revenues.write.mode(SaveMode.Append).cosmosDB(writeConfig)
    revenues.show(5)
    ```

47. On the **Insert** menu, click **Insert Cell Below**.
48. Add the following Scala code to the new cell. This code saves the dataframe that shows the popularity of products:

    ```Scala
    val itemsVolume = orderedByVolume.withColumn("doctype", functions.lit("MonthlySummary")).withColumn("month", functions.lit(monthYear)).withColumn("partitionKey", functions.lit("summary"))
    itemsVolume.write.mode(SaveMode.Append).cosmosDB(writeConfig)
    itemsVolume.show(5)
    ```

49. On the **Insert** menu, click **Insert Cell Below**.
50. Add the following Scala code to the new cell. This code displays a dataframe that shows the revenue generated by product. You do not need to save this data,a s it is the same as that generated by te previous dataframe, just in a different order:

    ```Scala
    val itemsRevenue = orderedByRevenue.withColumn("doctype", functions.lit("MonthlySummary")).withColumn("month", functions.lit(monthYear)).withColumn("partitionKey", functions.lit("summary"))
    itemsRevenue.show(5)
    ```

51. In the toolbar, click the **Save and checkpoint** icon.
52. On the **Cell** menu, click **Run All**. You should see the results output by each cell as it is executed.

    > **Note:** With the small-scale Spark configuation used by this lab (to save costs), the final few cells that write the data back to Cosmos DB can take some time to run. You should continue with the next exercise while the notebook executes, and come back to verify the results later.

53. When the notebook has completed, using the Azure portal go to your Cosmos DB account, select **Document Explorer**, select the **Adventure-Works** database, select the **Data** collection, and then click **New SQL Query**. Enter the following query, and then click **Execure Query**:

    ```SQL
    SELECT * FROM c WHERE c.doctype = "MonthlySummary"
    ```

54. Verify that a set of summary documents appear. The example below shows the typical documents showing the number of orders by order status:

    ```JSON
    [
        {
            "doctype": "MonthlySummary",
            "orderstatus": "Delivered",
            "month": "7/2018",
            "partitionKey": "summary",
            "count": 236,
            "id": "84c0b06a-4191-4859-b5d6-9cd434bd5a71",
            "_rid": "CyxpAK2n81T2HQAAAAAAAA==",
            "_self": "dbs/CyxpAA==/colls/CyxpAK2n81Q=/docs/CyxpAK2n81T2HQAAAAAAAA==/",
            "_etag": "\"0000ef1f-0000-0000-0000-5b6af6f00000\"",
            "_attachments": "attachments/",
            "_ts": 1533736688
        },
        {
            "doctype": "MonthlySummary",
            "orderstatus": "In progress",
            "month": "7/2018",
            "partitionKey": "summary",
            "count": 309,
            "id": "02727670-622b-420d-9790-36354c0b77a7",
            "_rid": "CyxpAK2n81T3HQAAAAAAAA==",
            "_self": "dbs/CyxpAA==/colls/CyxpAK2n81Q=/docs/CyxpAK2n81T3HQAAAAAAAA==/",
            "_etag": "\"0000f01f-0000-0000-0000-5b6af6f10000\"",
            "_attachments": "attachments/",
            "_ts": 1533736689
        },
        {
            "doctype": "MonthlySummary",
            "orderstatus": "Cancelled",
            "month": "7/2018",
            "partitionKey": "summary",
            "count": 46,
            "id": "e1f8ce67-749b-4cbd-abef-ec165b8fff3c",
            "_rid": "CyxpAK2n81T4HQAAAAAAAA==",
            "_self": "dbs/CyxpAA==/colls/CyxpAK2n81Q=/docs/CyxpAK2n81T4HQAAAAAAAA==/",
            "_etag": "\"0000f11f-0000-0000-0000-5b6af6f50000\"",
            "_attachments": "attachments/",
            "_ts": 1533736693
        },
        ...
    ]
    ```

## Exercise 3: Visualizing Sales Data

### Exercise 3 Scenario

You have been asked to create a dashboard that shows the sales volumes and revenues generated by product. You decide to implement this dashboard by using Azure Databricks, with Cosmos DB as the data source. To ensure that the system is scalable and responsive, you will use Spark to connect to Cosmos DB.

### Task 1: Create an Azure Databricks Service and Cluster

1. In the Azure Portal, click **Create a Resource**, type **Azure Databricks**, and then press Enter.
2. On the **Azure Databricks** blade, click **Create**.
3. On the **Azure Databricks Service** blade, in the **Workspace name** box, type **20777a-databricks-&lt;your name&gt;-&lt;the day&gt;**, for example, 20777a-databricks-john-31.
4. Select your Azure subscription.
5. In the **Resource group** box, click **Use existing**, and then click **20777Mod06**.

    > **Note:** The Azure Databricks service creates a significant number of resources. It places them in a seperate resource group with a name based on the name that you select here.

6. Set the **Location to West US 2**.

    > **Note:** Currently, not all regions support the range of VMs used by Azure Databricks to host Spark clusters. West US 2 does.

7. For the **Pricing Tier**, select **Trial**, and then click **Create**.
8. When the service has been deployed, click **All Resources**.
9. In the **All Resources** blade, click your **Azure Databricks** service (20777a-databricks-&lt;your name&gt;-&lt;the day&gt;).
10. On the **20777a-databricks-&lt;your name&gt;-&lt;the day&gt;** blade, click **Launch Workspace**.
11. On the **Azure Databricks** blade, under **Common Tasks**, click **New Cluster**.
12. On the **New Cluster** blade, for the **Cluster Name**, type **&lt;your name&gt;-cluster**.
13. Set **Max Workers** to 4, leave all other settings at their default values, and then click **Create Cluster**.
14. Wait while the cluster is created and started. Verify that its **State** is set to **Running** before continuing.

### Task 2: Create a Library and PyDocumentDB Notebook

1. In the toolbar to the left of the **Clusters** blade, click **Azure Databricks**.
2. On the **Azure Databricks** blade, under **Common Tasks**, click **Import Library**.
3. On the **Create Library** page, under **Library Source**, click **PyPI**.
4. In the **Package** box, type **pydocumentdb**, and then click **Create**.
5. On the **pydocumentdb** page, in the **\<*your name*\>-cluster** row, select the check box, click **Install**, and wait until the status changes to **Installed**.
6. In the left panel, click **Azure Databricks**.
7. On the **Azure Databricks** blade, under **Common Tasks**, click **Import Library**.
8. On the **Create Library** blade, under **Library Type**, click **Jar**.
9. In the **Library Name** box, type **Cosmos DB Connector**.
10. In the **JAR File** box, click **Drop JAR here**.
11. In the **Choose File to Upload** dialog box, go to **E:\Labfiles\Lab06\Starter\Exercise 2**, click **azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar**, and then click **Open**.
12. Wait for the JAR file to be uploaded, and then click **Create**.
13. On the **Cosmos DB Connector** page, select the **\<your name\>-cluster** check box, click **Install**, and then wait while the library is attached to the cluster; the **Status** should change to **Installed**.
14. In the toolbar to the left of the **pydocumentdb** blade, click **Azure Databricks**.
15. On the **Azure Databricks** blade, under **Common Tasks** click **New Notebook**.
16. In the **Create Notebook** dialog box, in the **Name** box, type **Analyze Orders**, set the **Language** to **Python**, and then click **Create**

    > **Note:** This exercise uses Python and PyDocumentDB, which is suitable for processing moderate amounts of data. If you have a large and growing volume of data, you should consider creating a Scala notebook with the Cosmos DB Connector, which can provide a more scalable solution.

### Task 3: Write Python Code to Retrieve and Display the Data

1. In the first cell of the notebook, enter the following code that specifies the modules that will be used by the notebook:

    ```Python
    # Import modules

    import pydocumentdb
    from pydocumentdb import document_client
    from pydocumentdb import documents
    import datetime
    from datetime import datetime
    ```

2. In the toolbar at the top right-hand side of the cell, click the **play** icon, and then click **Run Cell**. Verify that the code in the cell runs without any errors.
3. In the same toolbar, click the down-arrow icon, and then click **Add Cell Below**.

    ```Python
    # Work out what this time last month was, in ticks

    thisTimelastMonthInTicks = (datetime.utcnow() - datetime(1,1,1)).total_seconds() * 1000000
    ```

4. Run the cell.
5. Add another cell and enter the following code. This code defines the query that the notebook will use to retrieve the data from Cosmos DB.

    ```Python
    # Define the query to retrieve the orders data from Cosmos DB

    query = 'SELECT o.id, o.dateplaced, o.orderstatus, i.productnumber, i.productname, i.numberincartorordered, i.lineitemtotalcost FROM o JOIN i IN o.orderitems WHERE o.doctype = \\\'ShoppingCartOrder\\\' AND o.dateplaced > ' + str(thisTimelastMonthInTicks)
    ```

6. Run the cell.
7. Add another cell and enter the following code. Replace **--URI--** and **--KEY--** with the values for your Cosmos DB account. This code specifies the command that Spark will use to create a virtual table that it can use to fetch the data from Cosmos DB.

    ```Python
    # Specify the Spark SQL command to create a virtual table that references the data defined by the query in the Cosmos DB collection

    command = "create table orderdata using com.microsoft.azure.cosmosdb.spark options (endpoint '--URI--', masterkey '--KEY--', database 'Adventure-Works', collection 'Data', query_custom '" + str(query) + "')"
    ```

8. Run the cell.
9. Add another cell and enter the following code. This statement removes the **orderdata** table from the database if it already exists.

    ```Python
    # Remove any existing version of the orderdata table

    spark.sql('drop table if exists orderdata')
    ```

10. Run the cell.
11. Add another cell and enter the following code. This statement runs the **create table** command to recreate the **orderdata** table, using the latest value for **thisTimeLastMonthInTicks** (any previous version of the table would have been created using an older value for this variable, so would reference data more than a month old).

    ```Python
    # Recreate the orderdata table using the new query

    spark.sql(command)
    ```

12. Run the cell.
13. Add another cell and enter the following code. These statements create a Spark dataframe from the rows in the **orderdata** table, and display the first 20 rows.

    ```Python
    # Create a Spark dataframe using the records in the orderdata virtual table

    ordersdf = spark.sql("select * from orderdata")
    ordersdf.show()
    ```

14. Run the cell.

    > **Note:** You can view the progress of the Spark job that creates this dataset by expanding the **Spark Jobs** node under the cell as it runs. Click the (i) icon to see a detailed viewYou can also view the schema of the dataframe that is being created.

15. Add another cell and enter the following code. This code creates another Spark dataframe that summarizes the volume of sales and revenue generated for each product.

    ```Python
    # Create a dataframe that summarizes the data to show the total number of each product ordered, and the revenue generated by each product

    orderitemssummary = spark.sql("SELECT productnumber, productname, SUM(numberincartorordered) AS numordered, SUM(lineitemtotalcost) AS revenue FROM orderdata WHERE orderstatus <> 'Cancelled' GROUP BY productnumber, productname")
    orderitemssummary.show()
    ```

16. Run the cell.
17. Add another cell and enter the following code. These statements create a further dataframe that sorts the data by volume ordered and displays the results.

    ```Python
    # Sort the data by volume ordered for each product, and show the results in a bar chart

    productsByVolume = orderitemssummary.orderBy("numordered", ascending = False)
    display(productsByVolume)
    ```

18. Run the cell. The results should appear in a tabular format, with the most popular product at the top.
19. Click the graph icon below the table and select **Bar**, and then click **Plot Options**.
20. In the **Customize Plot** window, remove the fields from the **Keys** and **Values** boxes, drag the **productname** field to the **Series groupings** box, drag the **numberordered** field to the **Values** box, set **Aggregation** to **SUM**, and then click **Apply**. The bar chart should be displayed in the notebook.
21. Add another cell and enter the following code. These statements create a final dataframe that sorts the data by vrevenue and displays the results.

    ```Python
    # Sort the data by revenue for each product, and show the results in another bar chart

    productsByRevenue = orderitemssummary.orderBy("revenue", ascending = False)
    display(productsByRevenue)
    ```

22. Run the cell.
23. Click the graph icon below the table and select **Bar**, and then click **Plot Options**.
24. In the **Customize Plot** window, remove the fields from the **Keys** box (leave the **Values** box set to **Revenue**), drag the **productname** field to the **Series groupings** box, set **Aggregation** to **SUM**, and then click **Apply**.

### Task 4: Create a Dashboard

1. Return to the cell showing the bar chart that sorts orders by product volume.
2. In the toolbar at the top right-hand side of the final cell in the notebook, click the **Show in Dashboard Menu** icon (this icon has the image of a graph). Click **Add to New Dshboard**.
3. In the **Analyze Orders** blade showing the dashboard, change the title to **Monthly Orders Analysis**.
4. Resize the graph so that it occupies the entire presentation pane.
5. Click the **Analyze Orders** link adjacent to **View of notebook** under the dashboard title to return to the notebook.
6. Go to the final cell in the notebook, click **Show in Dashboard**, and then select the check box for **Monthly Orders Analysis**, and then click the **Go to Dashboard Monthly Orders Analysis** icon. You should see the revenue graph below the numberordered graph on te dashboard.
7. Resize the revenue graph to make it the same size as the numberordered graph.
8. Click **Run All** to test the notebook and dashboard. The data from the graphs will disappear while the notebook runs, and then reappear once the dataframes have been regenerated.

### Task 5: Cleanup the lab environment

- In the Azure portal, delete the **20777_Mod06** resource group.

---

 2019 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
