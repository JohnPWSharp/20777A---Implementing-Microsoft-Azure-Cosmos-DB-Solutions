# Module 4: Optimizing and Monitoring Performance

- [Module 4: Optimizing and Monitoring Performance](#module-4-optimizing-and-monitoring-performance)
    - [Lab: Tuning and Monitoring Performance in a Cosmos DB Application](#lab-tuning-and-monitoring-performance-in-a-cosmos-db-application)
        - [Lab Scenario](#lab-scenario)
        - [Objectives](#objectives)
        - [Lab Setup](#lab-setup)
    - [Exercise 1: Examining Cosmos DB execution statistics](#exercise-1-examining-cosmos-db-execution-statistics)
        - [Exercise 1 Scenario](#exercise-1-scenario)
        - [Task 1: Prepare the Environment](#task-1-prepare-the-environment)
        - [Task 2: Examine the data in the collection and assess the performance of common queries](#task-2-examine-the-data-in-the-collection-and-assess-the-performance-of-common-queries)
        - [Task 3: Gather performance statistics in an application](#task-3-gather-performance-statistics-in-an-application)
    - [Exercise 2: Monitoring Performance using the Azure Portal](#exercise-2-monitoring-performance-using-the-azure-portal)
        - [Exercise 2 Scenario](#exercise-2-scenario)
        - [Task 1: Create and populate collections](#task-1-create-and-populate-collections)
        - [Task 2: Run a workload that tests the performance of each partitioning scheme](#task-2-run-a-workload-that-tests-the-performance-of-each-partitioning-scheme)
        - [Task 3: Assess the results](#task-3-assess-the-results)
    - [Exercise 3: Assessing the Impact of Consistency Levels](#exercise-3-assessing-the-impact-of-consistency-levels)
        - [Exercise 3 Scenario](#exercise-3-scenario)
        - [Task 1: Create the AddPriceHistoryToDocument trigger](#task-1-create-the-addpricehistorytodocument-trigger)
        - [Task 2: Test the trigger using different consistency levels](#task-2-test-the-trigger-using-different-consistency-levels)
        - [Task 3: Replicate the database across regions, and retest the trigger](#task-3-replicate-the-database-across-regions-and-retest-the-trigger)
    - [Exercise 4: Investigating the Effects of Triggers on Performance](#exercise-4-investigating-the-effects-of-triggers-on-performance)
        - [Exercise 4 Scenario](#exercise-4-scenario)
        - [Task 1: Implement the CreatePriceHistoryDocument trigger](#task-1-implement-the-createpricehistorydocument-trigger)
        - [Task 2: Compare the performance of using the trigger to performing the same operation in a client application](#task-2-compare-the-performance-of-using-the-trigger-to-performing-the-same-operation-in-a-client-application)
        - [Task 3: Cleanup the lab environment](#task-3-cleanup-the-lab-environment)

## Lab: Tuning and Monitoring Performance in a Cosmos DB Application

### Lab Scenario

To enable more detailed reporting, Adventure Works pricing analysts want to keep historical product price information in Cosmos DB; they will access this information through a new application. You want to investigate the impact of different document schemas, query methods, and other factors that could affect the application's performance.

### Objectives

At the end of this lab, you should be able to:

1. Gather and examine Cosmos DB execution statistics.
2. Monitor the performance of a Cosmos DB database by using the Azure portal.
3. Assess the impact of the different consistency models on a Cosmos DB application.
4. Investigate the effects of triggers on performance.

### Lab Setup

Estimated time: **60 minutes**  
Virtual machine: **20777A-LON-DEV**  
User name: **LON-DEV\\Administrator**  
Password: **Pa55w.rd**

## Exercise 1: Examining Cosmos DB execution statistics

### Exercise 1 Scenario

To store product price history information, you have at least two options:

- Each time the price changes, create a new price history document for the product that contains the previous price together with the date in which it came into effect. The documents below show an example of this approach:

    ```JSON
    {
        "productid": "709",
        "subcategory": "Socks",
        "doctype": "Product",
        "productcategory": "Clothing",
        "productname": "Mountain Bike Socks, M",
        "productnumber": "SO-B909-M",
        "color": "White",
        "listprice": 9.5,
        "size": "M ",
        "weight": " ",
        "quantityinstock": 180,
        "model": "Mountain Bike Socks",
        "description": "Combination of natural and synthetic fibers stays dry and provides just the right cushioning.",
        "documentation": null,
        "images": null,
        "ttl": -1,
        "id": "709",
        "_rid": "TssuAK+uQZUDAAAAAAAAAA==",
        "_self": "dbs/TssuAA==/colls/TssuAK+uQZU=/docs/TssuAK+uQZUDAAAAAAAAAA==/",
        "_etag": "\"00006304-0000-0000-0000-5b7ed04c0000\"",
        "_attachments": "attachments/",
        "_ts": 1535037516
    },
    {
        "id": "709:Price:2009-02-01",
        "doctype": "ProductHistory",
        "subcategory": "Socks",
        "productid": "709",
        "listprice": "7.6000000000",
        "pricedate": "2009-02-01",
        "ttl": -1,
        "_rid": "TssuAK+uQZUEAAAAAAAAAA==",
        "_self": "dbs/TssuAA==/colls/TssuAK+uQZU=/docs/TssuAK+uQZUEAAAAAAAAAA==/",
        "_etag": "\"00006404-0000-0000-0000-5b7ed04c0000\"",
        "_attachments": "attachments/",
        "_ts": 1535037516
    },
    {
        "id": "709:Price:2009-03-01",
        "doctype": "ProductHistory",
        "subcategory": "Socks",
        "productid": "709",
        "listprice": "9.0250000000",
        "pricedate": "2009-03-01",
        "ttl": -1,
        "_rid": "TssuAK+uQZUFAAAAAAAAAA==",
        "_self": "dbs/TssuAA==/colls/TssuAK+uQZU=/docs/TssuAK+uQZUFAAAAAAAAAA==/",
        "_etag": "\"00006504-0000-0000-0000-5b7ed04c0000\"",
        "_attachments": "attachments/",
        "_ts": 1535037516
    },
    ...
    ```

- Store the price history as part of the product document:

    ```JSON
    {
        "id": "875-with-price-history",
        "productid": "875",
        "subcategory": "Socks",
        "doctype": "ProductWithPriceHistory",
        "productcategory": "Clothing",
        "productname": "Racing Socks, L",
        "productnumber": "SO-R809-L",
        "color": "White",
        "listprice": 8.99,
        "size": "L ",
        "weight": " ",
        "quantityinstock": 288,
        "model": "Racing Socks",
        "description": "Thin, lightweight and durable with cuffs that stay up.",
        "pricehistory": [
            {
                "productid": "875",
                "listprice": "10.4284000000",
                "pricedate": "2009-01-01"
            },
            {
                "ProductID": "875",
                "listprice": "8.0910000000",
                "pricedate": "2009-02-01"
            },
            {
                "ProductID": "875",
                "listprice": "9.5294000000",
                "pricedate": "2009-03-01"
            },
            {
                "ProductID": "875",
                "listprice": "8.9001000000",
                "pricedate": "2009-04-01"
            },
            ...
        ]
    }
    ```

Each approach is likely to have benefits and drawbacks. In the first case, you can end up with a lot of documents. To find the complete price history for a product could involve retrieving all of these documents. In the second case, it is easier to retrieve the full price history, but each time you fetch the details for a single product you might end up retrieving a lot of extraneous data as well.

You want to assess the possible impact of both of these strategies on your applications.

The main tasks for this exercise are as follows:

1. Prepare the environment.
2. Examine the data in the collection and assess the performance of common queries.
3. Gather performance statistics in an application.

### Task 1: Prepare the Environment

1. Ensure that the **MT17B-WS2016-NAT** and **20777A-LON-DEV** virtual machines are running, and then log on to **20777A-LON-DEV** as **LON-DEV\\Administrator** with the password **Pa55w.rd**.
2. In File Explorer, navigate to **E:\\Labfiles\\Lab04\\Starter**, right-click **full-cosmos-setup.ps1**, and then click **Edit**.
3. On line 2 of the PowerShell script, replace the text **&lt;your initials&gt;** with your initials, and replace **&lt;day&gt;** with the day of the month.
4. On line 3, change the **resourceGroupLocation** variable to reference your nearest location.
5. On the **File** menu, click **Save**.
6. On the **File** menu, click **Exit**.
7. In File Explorer, in the **E:\\Labfiles\\Lab04\\Starter** folder, right-click **Setup.cmd**, and then click **Run as administrator**.
8. When prompted enter your Azure credentials.
9. On the toolbar, click **Internet Explorer**.
10. In Internet Explorer, go to **http://portal.azure.com**, and sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
11. In the Azure portal, in the left panel, click **All resources**, and then click **20777-mod4-sql-&lt;your initials and day&gt;**.
12. On the **20777-mod7-sql-&lt;your initials and day&gt;** blade, under **Settings**, click **Keys**.
13. Make a note of the **URI**, and **PRIMARY KEY** values.

### Task 2: Examine the data in the collection and assess the performance of common queries

1. In the Azure portal, go to your Cosmos DB account, and open **Data Explorer**.
2. In the data explorer toolbar, click **Settings**, and set the **Query results per page** option to **600**.
3. Select the **Data** collection in the **Adventure-Works** database and run the following query. This query should return 4 documents; one for each type of sock that Adventure-Works sells.

    ```SQL
    SELECT * FROM c WHERE c.subcategory = "Socks" AND c.doctype = "Product"
    ```

4. Change the query as shown below and run it. This query should return 108 documents, each one showing the price of product 709 (one of the "sock" products) at some point in the past. Notice that to get the price history in sequence, you must order the data:

    ```SQL
    SELECT * FROM c WHERE c.subcategory = "Socks" AND c.doctype = "ProductHistory" AND c.productid = "709" ORDER BY c.pricedate
    ```

5. Click **Query Stats**, and make a note of the **Request Charge** showing the number of RUs required to perform this query.
6. Modify the query again, as follows, run it. This time you should see only a single document containing the product details and its full price history. Notice that the items in the **pricehistory** array are already stored in order, so you don't need to sort them as part of the query:

    ```SQL
    SELECT * FROM c WHERE c.subcategory = "Socks" AND c.doctype = "ProductWithPriceHistory" AND c.productid = "709"
    ```

7. Examine the **Query Stats** page, and compare the **Request Charge** to that of the previous query. It should be significantly lower.
8. Amend the query again and run it. This query should return the details of a single product from a **ProductWithPriceHistory** document (the price history is not included in the output).

    ```SQL
    SELECT c.productcategory, c.productname, c.productnumber, c.color, c.listprice, c.size, c.quantityinstock FROM c WHERE c.subcategory = "Socks" AND c.doctype = "ProductWithPriceHistory" AND c.productid = "709"
    ```

9. Examine the **Query Stats**, and note the **Request Charge**.
10. Change the query once more, and run it. This query should return the same data as before, but using the **Product** document instead.

    ```SQL
    SELECT c.productcategory, c.productname, c.productnumber, c.color, c.listprice, c.size, c.quantityinstock FROM c WHERE c.subcategory = "Socks" AND c.doctype = "Product" AND c.productid = "709"
    ```

11. Examine the performance figures in the **Query Stats** blade, and compare the **Request Charge** to that of the previous query. It should be less than that of the previous query. This is because the amount of I/O that Cosmos DB had to perform to fetch the document is much lower due to the document being much smaller.

### Task 3: Gather performance statistics in an application

1. On the deskstop, start Visual Studio, and open the **ProductCatalogPerformance** solution in the **E:\\Labfiles\\Lab04\\Starter\\Exercise01\\ProdoductCatlogPerformance** folder. This app enables you to perform a number of queries for product information, to test the performance of the different ways of structuring the product documents.
2. Open the **Program.cs** file.
3. Locate the **FindProductDocumentByID** method. This method retrieves a product by using the **ReadDocumentAsync** method. **ReadDocumentAsync** requires you to specify the document Uri, and the partition key for the partition containing the document. It provides the fastest access to a document in a collection.
4. In the **FindProductDocumentByID** method, under the comment **TODO write out request change and request latency**, add the following code. This code prints the request charge and time taken to the console:

    ```Csharp
    Console.WriteLine($" Request charge: {documentResponse.RequestCharge} RUs \n Request Latency: {documentResponse.RequestLatency} ms");
    ```

5. Locate the **RunAQuery** method.This method prompts the user to enter an abitrary query and then runs it (no checks are performed to ensure tha the query is valid). The query is performed with the **EnableCrossPartitionQuery** option set to true.
6. Locate the **FindProductDocumentByQueryId** method. This method retrieves a document by performing an SQL query. Notice that the FeedOptions used by this query enable cross partition searches, and will scan every partition in the database to find matching documents.
7. Locate the **FindProductDocumentByQueryIdAndPartition** method. This method is similar to the the **FindProductDocumentByQueryId** method, except that th query it runs expects you to provide a partition key, and the query will only search the specified partition.
8. Find the **OutputProductResult** method. This method runs a specified query and displays the results.
9. In the **OutputProductResult** method, under the comment **TODO create local variables to aggregate metrics** add the following code to initialize the local variables that you will populate with performance information:

    ```CSharp
    double requestCharge = 0;
    double indexLookupTime = 0;
    double indexHitRatio = 0;
    double documentLoadTime = 0;
    double runtimeExecutionTimes = 0;
    ```
10. In the same method, under the comment **TODO aggregate metrics** add the following code to aggregate the metrics to the local variables that you set up in the previous step:

    ```Csharp
    requestCharge += queryResponse.RequestCharge;
    if (queryResponse.QueryMetrics.Count() > 0)
    {
        var queryMetrics = queryResponse.QueryMetrics.First().Value;
        indexLookupTime += queryMetrics.QueryEngineTimes.IndexLookupTime.TotalMilliseconds;
        indexHitRatio = queryMetrics.IndexHitRatio;
        documentLoadTime += queryMetrics.QueryEngineTimes.DocumentLoadTime.TotalMilliseconds;
        runtimeExecutionTimes += queryMetrics.QueryEngineTimes.RuntimeExecutionTimes.TotalTime.TotalMilliseconds;
    }
    ```

11. Under the comment **TODO Display metrics** add the following code to print the metrics to the console:

    ```Csharp
    Console.WriteLine($"\n Request charge: {requestCharge} RUs\n Index LookUp Time: {indexLookupTime} ms\n Index Hit Ratio: {indexHitRatio * 100}%\n Document Load Time: {documentLoadTime} ms\n Runtime Execution Time: {runtimeExecutionTimes} ms\n");
    ```

12. Edit the **App.config** file. In the **appSettings** section, change the value of the **EndpointUrl** and **PrimaryKey** settings to the URI and primary key for your Cosmos DB account that you recorded in task 1.
13. Build and run the application.
14. At the prompt press **A** to retrieve a document by id/subcategory using the **ReadDocumentAsync** method. Specify product id **709** and partition key **Socks** when prompted. Note the value of the request charge for the query.
15. At the prompt press **B** to retrieve a document with a query by id only (a cross partition query). Specify product id **709** and subcategory **Socks** again. Notice that the request charge is much higher than that of the partitioned query run by using the **ReadDocumentAsync** method.
16. At the prompt press **C** to retrieve a document with a query by id and partition key. Use product id **709** again, and specify a partition key of **Socks**. Observe that the RU cost is much lower than the previous example, but not as low as that of the **ReadDocumentAsync** method.
19. To compare the effect of embedding the price history information in the product documents with holding the price history in individual documents on queries that read product information, at the prompt press **R** to invoke the **RunAQuery** method, and then enter the following query:

    ```SQL
    SELECT c.id, c.productcategory, c.productname, c.productnumber, c.listprice FROM c where c.doctype = 'Product'
    ```
20. Note the metrics.
21. At the prompt, press **R** again, and then enter the following query:

    ```SQL
    SELECT c.id, c.productcategory, c.productname, c.productnumber, c.listprice FROM c where c.doctype = 'ProductWithPriceHistory'
    ```

22. Compare the metrics from both queries. Observe that the larger documents (the second query) have a higher RU cost to return the same information (~20-25%). The Document Load Time has more than doubled.

    **Question**: What conclusions do you draw from these results?

23. To compare the performance of retrieving price history information, at the prompt, press **R**, and then run the following query:

    ```SQL
    select c.productid, c.listprice, c.pricedate from c where c.doctype = 'ProductHistory'
    ```

24. Note the metrics.
25. Press **R**, and run the following query:

    ```SQL
    SELECT x.productid, x.listprice, x.pricedate FROM c JOIN x IN c.pricehistory where c.doctype = 'ProductWithPriceHistory'
    ```

26. Compare the metrics from both queries. Observe that the RU cost of the second query is ~30% lower. The Document Load Time and Runtime Execution Time are also significantly lower.

    **Question**: What conclusions do you draw from these results? What changes might you implement to improve the performance of retrieving price history data?

27. Press **X** to exit the application.
28. In Visual Studio, in the Program.cs  file, find the **CreateProductHistories** method. This method is not currently implemented. Replace the existing statement in this method with the following code. This code creates seperate product history documents from the prices recorded in the price history array of the **ProductWithPriceHistory** documents.

    ```CSharp
    private async Task CreateProductHistories()
    {
        SqlQuerySpec querySpec = new SqlQuerySpec()
        {
            QueryText = "SELECT * FROM c WHERE c.doctype = 'ProductWithPriceHistory'"
        };

        FeedOptions queryOptions = new FeedOptions { MaxItemCount = -1, EnableCrossPartitionQuery = true, PopulateQueryMetrics = this.enableLogging };
        var productWithHistoryQuery = client.CreateDocumentQuery<ProductWithHistory>(UriFactory.CreateDocumentCollectionUri(this.database, this.collection), querySpec, queryOptions).AsDocumentQuery();

        while (productWithHistoryQuery.HasMoreResults)
        {
            var queryResponse = (productWithHistoryQuery.ExecuteNextAsync()).Result;
            foreach (var queryDoc in queryResponse)
            {
                dynamic productHistories = new System.Dynamic.ExpandoObject();
                productHistories.productid = queryDoc.productid;
                productHistories.pricehistory = queryDoc.pricehistory;
                productHistories.doctype = "ProductHistories";
                productHistories.subcategory = queryDoc.subcategory;

                var createResponse = await client.CreateDocumentAsync(UriFactory.CreateDocumentCollectionUri(this.database, this.collection), productHistories);
            }
        }
    }
    ```

29. Build and run the application. 
30. At the prompt press **D** to create the **ProductHistories** records for each product in the collection. This operation will take a little time.
31. To compare the performance of retrieving price history information, at the prompt, press **H** to prevent results being displayed, and then press **R**.
32. Run the following query:

    ```SQL
    SELECT x.productid, x.listprice, x.pricedate FROM c JOIN x IN c.pricehistory WHERE c.doctype = 'ProductHistories'
    ```

33. Compare the metrics with the product history query metrics that you recorded earlier in the exercise. Notice that the **ProductHistories** query has similar metrics to the **ProductWithPriceHistory** query. This approach gives you good performance for both price history and standard product catalog queries.
34. Press **X** to close the app, and then close Visual Studio.

## Exercise 2: Monitoring Performance using the Azure Portal

### Exercise 2 Scenario

The addition of product price histories into the database has lead you to consider reassessing the partitioning scheme used by the system. You have decided to test the relative performance of the following partitioning strategies for storing product and product history documents:

- Partition by subcategory (the current scheme).
- Partition by product id.
- Partition by doc type.
- No partitioning.

The main tasks for this exercise are as follows:

1. Create and populate collections for each partitioning strategy.
2. Run a workload that tests the performance of each scheme.
3. Assess the results.

### Task 1: Create and populate collections

1. In the Azure portal, go to your Cosmos DB account.
2. Using **Data Explorer**, delete the **Data** collection in the **Adventure-Works** database.
3. On the desktop, go to the **E:\\Labfiles\\Lab04\\Starter** folder, and edit the PowerShell script **create-database-partitioned-by-subcategory.ps1** by using PowerShell ISE.
4. In the script, change the value of the **\$CosmosDbEndpoint** variable to match the value of the Cosmos DB URI, and change the value of the **\$CosmosDbPrimaryKey** variable to the primary key of your Cosmos DB account.
5. Run the script. It will recreate the **Data** collection (partitioned by subcategory) and upload 34344 documents.
6. Open the **create-database-partitioned-by-productid.ps1** file, set the **\$CosmosDbEndpoint** and **\$CosmosDbPrimaryKey** variables, and run the script. This script creates another collection named **Data2** that is partitioned by productid and uploads the same 34344 documents.
7. Open the **create-database-partitioned-by-doctype.ps1** file, set the **\$CosmosDbEndpoint** and **\$CosmosDbPrimaryKey** variables, and run the script. This script creates a futher collection named **Data3** that is partitioned by doctype and uploads the documents.
8.  Open the **create-database-not-partitioned.ps1** file, set the **\$CosmosDbEndpoint** and **\$CosmosDbPrimaryKey** variables, and run the script. This script creates a final collection named **Data4** that is not partitioned, and uploads the documents.
9.  Return to the Azure portal and go to **Data Explorer** for your Cosmos DB account. Refresh the blade if necssary to see the new collections.
10. Verify that the database contains four collections, named **Data**, **Data2**, **Data3**, and **Data4**. These are the collections created by the PowerShell scripts. Data is the collection partitioned by subcategory, Data2 is partitioned by productid, Data3 is partitioned by doctype, and Data4 is not partitioned.
11. For each collection, verify thet that allocated throughput is 5000 RU/s. For the partitioned collections, this throughput will be shared across five partion key ranges, each being allocated 1000 RU/s.

### Task 2: Run a workload that tests the performance of each partitioning scheme

1. Usingt Visual Studio, and open the **QueryPerformanceTester** solution in the **E:\\Labfiles\\Lab04\\Starter\\Exercise02\\QueryPerformanceTester** folder. This app simulates multiple users performing a number of queries against each of the collections.
2. Open the **Program.cs** and find the **FindProductDocuments** method. This method runs the following query to find the documents for a product given the subcategory and productid. Note that both of these parameters are necessary to satisfy the requirements for thecollections partitioned by subcategory and productid; it is important to run the same query across all collections to get a true comparison of performance:

    ```SQL
    SELECT * FROM c WHERE c.subcategory = @subcategory AND c.productid = @productid AND (c.doctype = 'Product' OR c.doctype = 'ProductHistory')
    ```

3. Scroll up to the **DoWork** method. This method connects to the Cosmos DB database, and the invokes the **FindProductDocuments** method repeatedly, specify each of the collections in turn. The code that does this is highlighted below. The boolean parameter passed to the **FindProductDocuments** method indicates whether the method should enable cross-partition queries. This parameter is only set to true for the query that retrieves data from the collection partitioned by doctype (product and product price history documents are stored in different partitions):

    ```CSharp
    int numIterations = int.Parse(ConfigurationManager.AppSettings["NumIterations"]);
    for (int i = 0; i < numIterations; i++)
    {
        Console.WriteLine($"Worker{workerNum}, Iteration {i}");
        FindProductDocuments(collectionPartitionedBySubcategory, false);
        FindProductDocuments(collectionPartitionedByProductID, false);
        FindProductDocuments(collectionPartitionedByDocType, true);
        FindProductDocuments(collectionNotPartitioned, false);
    }
    ```

4. Edit the **App.config** file.In the **appSettings** section, change the value of the **EndpointUrl** and **PrimaryKey** settings to the URI and primary key for your Cosmos DB account.
5. Build and run the application. The application will take 15-20 minutes to complete. Make a note of the start and end times reported by the application. Don't close the window because you will need these results later.

### Task 3: Assess the results

1. Return to your Cosmos DB account in the Azure portal and display the **Metrics** blade.
2. On the **Storage** tab, display the storage information for the **Data** collection in the **Adventure-Works** database. This is the collection partitioned by subcategory.
3. In the **Data + Index storage consumed by top partition keys** graph, observe that there is significant skew in the partition sizes.
4. Click the biggest partition; the partition key should be **Road Bikes**.
5. Click the biggest partition again to deselect it, and then click the second biggest partition; the partition key should be **Mountain Frames**.

    **Question**: What is the principal drawback of having partitions skewed in this way, and what has caused it in this case?
  
6. At the bottom of the blade, in the **Data + Index storage consumed per partition key range** graph, click the biggest partition range.

    **Question**: The data for which partitions is stored in this partition key range?

7. Display the storage information for the **Data2** collection. This is the collection partitioned by productid.
8. In the **Data + Index storage consumed by top partition keys** graph, notice that this partitioning scheme has distributed the data more evenly across the logical partitions.
9. At the bottom of the blade, in the **Data + Index storage consumed per partition key range** graph, again notice that the size of the partition key ranges are more even than the previous scheme.
10. Display the storage information for the **Data3** collection. This is the collection partitioned by doctype.
11. In the **Data + Index storage consumed by top partition keys** graph. This time, the partitons are very skewed, with the vast majority of documents in the **ProductHistory** partition. Remember from exercise 1 that a new  **ProductHistory** document is created every time a product has a price change, so this skew is unlikely to change and will probably only get worse.
12. Display the storage information for the **Data4** collection. This is the non-partitioned collection.
13. In the **Data + Index storage consumed by top partition keys** graph, notice that data is held in a single unamed partition.
14. Click the **Throughput** tab, and display the throughput statistics for the **Data** collection in the **Adventure-Works** database.
15. In the timeline toolbar, click **Custom**.
16. Set the **Start** time to the start time displayed by the **Run complete** message when you ran the application in task 2, and set the **End** time to the end time reported by the application.
17. In the **Number of requests (aggregated over 1 minute interval)** graph, note the average number of requests reported per minute while the app was running; it should be just over 1K.
18. In the **Max consumed RU/s per partition key range** graph, note the average RU/s for each partition key range. There may be some spikes, but it will typically hover around 500 (0.5K) most of the time.
19. Where you do get any spikes, click the spike on the graph. The **Max consumed RU/s by each partition key range** chart below the graph will show how the partitions were being accessed at that point in time.

    > **Note:** Recall that each collection is provisioned by 5000 RU/s, and these are allocated evenly across 5 partition key ranges (each partition key range gets 1000 RU/s). At some points in time, you might see that the throughput exceeds the allocated RU/s for a partition key range. This can lead to the database responding with HTTP 429 messages (Request rate is large). If this occurs frequently in a live system, you have either inot configured enough resources (in which case, scale the collection and add more throughput resources), or it could be caused by a skewed partition scheme. The **Max consumed RU/s by each partition key range** chart will indicate whether other partition key ranges have spare throughput capacity.

20. Display the throughput statistics for the **Data2** collection.
21. Verify that the **Number of requests (aggregated over 1 minute interval)** graph shows a similar number of requests reported per minute to this partition while the app was running; it should be just over 1K.
22. In the **Max consumed RU/s per partition key range** graph, you should see that the more even distribution provided by this partitioning scheme has prevented any single partition from exceeding its throughput resources. The average figure should be somewhere around 0.4K RU/s per partition key range.
23. Display the throughput statistics for the **Data3** collection.
24. Again, verify that the **Number of requests (aggregated over 1 minute interval)** graph shows a similar number of requests reported per minute to this partition while the app was running; it should be just over 1K.
25. In the **Max consumed RU/s per partition key range** graph, this time you should see that this partitioning scheme consumes far more resources than the previous one (probably close to double). The average figure should be somewhere around 0.8K RU/s per partition key range.
26. Click one of the peaks, and then examine the **Max consumed RU/s by each partition key range** chart.

    **Question**: What do you notice about this chart?

27. Display the throughput statistics for the **Data4** collection..
28. In the **Max consumed RU/s per partition key range** graph, this time you should see that this partitioning scheme consumes even more resources than the previous one per partition key range (somewhere around 1K). However, because this collection is not partitioned, the entire 5000 RU/s throughput is allocated to the single partition key range, so it doesn't exceed the available RU/s resources. You should also bear in mind that although this scheme is using only a moderate amount of throughput, it does not mean that it is the most optimal approach. This strategy is likely to cause contention (which isn't displayed by the graphs) resulting in longer response times, and is not scalable.

    **Question**: Given the findings displayed by the **Metrics** blade, which partitioning scheme appears to be the most optimal for storing product and product price history documents?

29. Close the **QueryPerformanceTester** app, and then close Visual Studio.

## Exercise 3: Assessing the Impact of Consistency Levels

### Exercise 3 Scenario

As part of the investigations on the optimal manner to store and maintain product price histories, you decide to investigate the **ProductWithPriceHistory** model a little further. Remember that documents using this model look like this:

```JSON
{
    "id": "875-with-price-history",
    "productid": "875",
    "subcategory": "Socks",
    "doctype": "ProductWithPriceHistory",
    "productcategory": "Clothing",
    "productname": "Racing Socks, L",
    "productnumber": "SO-R809-L",
    "color": "White",
    "listprice": 8.99,
    "size": "L ",
    "weight": " ",
    "quantityinstock": 288,
    "model": "Racing Socks",
    "description": "Thin, lightweight and durable with cuffs that stay up.",
    "pricehistory": [
        {
            "productid": "875",
            "listprice": "10.4284000000",
            "pricedate": "2009-01-01"
        },
        {
            "ProductID": "875",
            "listprice": "8.0910000000",
            "pricedate": "2009-02-01"
        },
        {
            "ProductID": "875",
            "listprice": "9.5294000000",
            "pricedate": "2009-03-01"
        },
        {
            "ProductID": "875",
            "listprice": "8.9001000000",
            "pricedate": "2009-04-01"
        },
        ...
    ]
}
```

You decide to use a **Pre** trigger that runs every time the price in a **ProductWithPriceHistory** document is updated. This trigger captures the current price (before the change), and creates and appends the appropriate sub-document to the **pricehistory** field. You want to verify that this approach will work as expected across the different consistency models that might be required by applications using the database.

The main tasks for this exercise are as follows:

1. Create the **AddPriceHistoryToDocument** trigger.
2. Test the trigger using different consistency levels.
3. Replicate the database across regions, and retest the trigger.

### Task 1: Create the AddPriceHistoryToDocument trigger

> **Note:** The JavaScript code used in this task can be found in the file **E:\\Labfiles\\Lab04\\\Solution\\Exercise03\\AddPriceHistoryToDocument.txt**.

1. In the Azure portal, go to your Cosmos DB account in the Azure portal.
2. Open **Data Explorer**, and select the **Data2** collection in the **Adventure-Works** database. This is the collection that is partitioned by product id.
3. Add a new trigger named **AddPriceHistoryToDocument** to the collection. Leave the **Trigger Type** set to **Pre**, and the **Trigger Operation** set to **All**.
4. Add the following code to the trigger, and then save it:

    ```JavaScript
    function trigger(){
        var context = getContext();  
        var request = context.getRequest();
        var collection = context.getCollection();  
        var documentBeingUpdated = request.getBody();

        // If the operation being performed is not an upsert, then ignore this trigger
        if (request.getOperationType() != "Upsert") {
            return;
        }

        // Verify the type of the document - should be a ProductWithPriceHistory
        if (!("doctype" in documentBeingUpdated) && documentBeingUpdated.doctype != "ProductWithPriceHistory") {
            throw new Error("Wrong type of document");
        }

        // Create a pricehistory subdocument using the data from the document triggering the operation
        var newPriceHistorySubDoc =
        {
            productid: documentBeingUpdated.productid,
            listprice: documentBeingUpdated.listprice,
            pricedate: new Date().toISOString().slice(0,10)
        };

        // Add the pricehistory subdocument to start of the pricehistory array
        documentBeingUpdated.pricehistory.push(newPriceHistorySubDoc);

        // Save the changes
        request.setBody(documentBeingUpdated);
    }
    ```


### Task 2: Test the trigger using different consistency levels

1. Using the Azure portal, examin the default consistency model used by the Cosmos DB account; it should be **BOUNDED STALENESS**. Applications can override this consistency level to lower it, but canot increase it to **STRONG**.
2. On the deskstop, start Visual Studio, and open the **PriceChangeTester** solution in the **E:\\Labfiles\\Lab04\\Starter\\Exercise03\\PriceChangeTester** folder. 
3. Edit the **Program.cs** file and locate the **DoWork** method in the **Worker** class. This method connects to the database, reads the document for a specified product, changes the price (it adds 0.5), and then saves it back to the collection, firing the **AddPriceHistoryToDocument** trigger as it does so. Note that the update is protected by using the **ETag** to prevent any concurrent changes from being lost, as follows. If a concurrent change is detected, the app throws an exception which is caught and reported by the handler at the end of the method:

    ```CSharp
    // Save the doc back to the collection
    options = new RequestOptions
    {
        AccessCondition = new AccessCondition
        {
            Condition = document.ETag,
            Type = AccessConditionType.IfMatch
        },
        PreTriggerInclude = new List<string>{ "AddPriceHistoryToDocument" }
    };

    var updateResponse = await client.UpsertDocumentAsync(
        UriFactory.CreateDocumentCollectionUri(this.database, this.collection), document, options);
    Console.WriteLine("Document updated");
    ```

4. Scroll up to the **Main** method of the **Program** class. This method prompts the user for a product id, and then runs the **DoWork** method of a **Worker** object 100 times in quick succession. These runs are all performed sequentially rather than concurrently; only one update to the specified product document should occur at a time:

    ```CSharp
    static void Main(string[] args)
    {
        Console.WriteLine("Enter product ID");
        string id = Console.ReadLine();

        Worker worker = new Worker();
        for (int i = 0; i < 100; i++)
        {
            worker.DoWork(id).Wait();
        }

        Console.WriteLine("Press Enter to finish");
        Console.ReadLine();
    }
    ```

5. Edit the **App.config** file, and add the URI and primary ket to the **appSettings** section where indicated.
6. Build and run the application.
7. At the **Enter product ID** prompt, enter 717. This will cause the price of product 717 to be updated. The app displays the complete document each time it is updated, and you should see the new price being appended to the array at the end of the document.
8. Verify that the app runs without reporting any exceptions, and then press Enter to close it.
9. Return to Vsual Studio.
10. In the **Program.cs** file, in the **DoWork** method, locate the following statement. This code creates the client object that connects to the database, notice that it specified a consistency level of **BoundedStaleness**:

    ```CSharp
    // Connect to the Cosmos DB account
    this.client = new DocumentClient(new Uri(endpointUrl), primaryKey, null, ConsistencyLevel.BoundedStaleness);
    ```

11. Change the consistency level to **ConsistencyLevel.Eventual**.
12. Build and run the application again.
13. At the **Enter product ID** prompt, enter 718.
14. Verify that the app operates as before, except that it is modifying the price of a different product.
15. Verify that the app runs without reporting any exceptions, and then press Enter to close it.
16. Return to Vsual Studio.
17. Repeat steps 10 to 16, using the following consistency levels. In each case, the app should run successfully:
    - **ConsistencyLevel.Session**
    - **ConsistencyLevel.ConsistentPrefix**
  
### Task 3: Replicate the database across regions, and retest the trigger

1. In the Azure portal, on the blade displaying the default consistency for your Cosmos DB account, verify that it is still set to **BOUNDED STALENESS**.
2. Change the **Maximum Lag(Time)** to 5 minutes, the **Maximum Lag(Operations)** to **100000**, and save the changes.

   > **Note:** These changes are necessary as they are the minimum values supported by the bounded staleness consistency level when you replicate data across regions.

3. Under **Settings**, click **Replicate data globally**.
4. On the **Replicate data globally** blade, click a location that is physically distant from your write region. For example, if your write region is **Central US**, click **Southeast Asia**.
5. Save the configuration and wait while the replication is configured. This can take several minutes.
6. Return to Visual Studio.
7. In the **Program.cs** file, in the **DoWork** method, change the consistency level used by the application back to **BoundedStaleness** as follows:

    ``` CSharp
    // Connect to the Cosmos DB account
    this.client = new DocumentClient(new Uri(endpointUrl), primaryKey, null, ConsistencyLevel.BoundedStaleness);
    ```

8. Build and run the application.
9. At the **Enter product ID** prompt, enter 725.
10. Verify that the app runs without reporting any exceptions, and then press Enter to close it.
11. Return to Vsual Studio.
12. In the **Program.cs** file, in the **DoWork** method, change the consistency level used by the application back to **Eventual** as follows:

    ``` CSharp
    // Connect to the Cosmos DB account
    this.client = new DocumentClient(new Uri(endpointUrl), primaryKey, null, ConsistencyLevel.Eventual);
    ```

13. Build and run the application again.
14. At the **Enter product ID** prompt, enter 726. The application will start updating documents, but at some point will throw the following exception. 

    ```Text
    Message: {"Errors":["One of the specified pre-condition is not met"]}
    ActivityId: 5e601622-0365-4bc0-a6c7-62c47c2ae7c1, Request URI: /apps/3666e0f0-ed7d-4e2e-8558-f69907b4ff5d/services/6e89fcb4-cbba-4b28-81b0-4714995eb1e6/partitions/0b3a4866-d542-4610-b3f3-c814312956c0/replicas/131798616051453023p, RequestStats:
    RequestStartTime: 2018-08-29T10:56:36.7260805Z, Number of regions attempted: 0
    , SDK: Microsoft.Azure.Documents.Common/2.0.0.0, Windows/10.0.17134 documentdb-netcore-sdk/1.9.1
    ```

    This exception occurs when the ETag for a document being updated is different from that expected. This situation typically occurs when two users are attempting to update the same document at the same time, and the solution for the app is to read the document again to retrieve the latest version of the document containing the latest ETag, update it, and store it back in the database (this is the optimistic concurrent approach). But in this case, there is only one user, and that user is performing updates serially. The problem is caused by the replication configuration. When the client app updates a document, not all copies of that document across all nodes are changed immediately. This is performance reasons. The changes will *eventually* be propagated. If the app is using the bounded staleness consistency level this is not a problem and the app will be presented with the most recent version of a document (within a given timeframe). However, this takes a little time as the system has to ascertain what the latest version actually is. The eventual consistency model simply fetches any copy of the document regardless of its version, and this might not be the latest version. If the app is presented with an outdated version of the document, modifies it, and then writes it back to the database, it could lose one or more changes. Using the ETag to check the version of the document prevents this from occurring. Note that this is not an issue with using triggers, rather it is a feature of any system that implements eventual consistency
15. Press Enter and allow the app to continue. It will likely trigger more exceptions as the same problem recurs.
16. When the app has finished, press Enter to close the app, and then close Visual Studio.

## Exercise 4: Investigating the Effects of Triggers on Performance

### Exercise 4 Scenario

You now decide to investigate the efficiency of storing product price history information as a series of seperate documents as an alternative to using an array of prices stored with the product itself. Each time the price of a product changes, a new **PriceHistory** document  should be created that looks similar to this:

```JSON
{
    "id": "737:Price:2009-01-01",
    "doctype": "ProductHistory",
    "subcategory": "Road Frames",
    "productid": "737",
    "listprice": "269.7760000000",
    "pricedate": "2009-01-01",
    ...
}
```

You want to determine whether to use a **Pre** trigger that creates a new **ProductHistory** document every time the price in a **Product** document changes, or whether it is more efficient to perform this task explicitly in a client application. 

The main tasks for this exercise are as follows:

1. Implement the **CreatePriceHistoryDocument** trigger.
2. Compare the performance of using the trigger to performing the same operation in a client application.
3. Cleanup the lab environment.

### Task 1: Implement the CreatePriceHistoryDocument trigger

> **Note:** The JavaScript code used in this task can be found in the file **E:\\Labfiles\\Lab04\\\Solution\\Exercise04\\CreatePriceHistoryDocument.txt**.

1. In the Azure portal, go to your Cosmos DB account, and use **Data Explorer** to navigate to the **Data2** collection in the **Adventure-Works** database
2. Create a new trigger named enter **CreatePriceHistoryDocument**. Leave the **Trigger Type** set to **Pre**, and the **Trigger Operation** set to **All**.
3. Change the code in the **Trigger Body** box as shown below:

    ```JavaScript
    function trigger(){
        var context = getContext();  
        var request = context.getRequest();
        var collection = context.getCollection();  
        var documentBeingUpdated = request.getBody();
 
        // If the operation being performed is not an upsert, then ignore this trigger
        if (request.getOperationType() != "Upsert") {
            return;
        }

        // Verify the type of the document - should be a Product
        if (!("doctype" in documentBeingUpdated) && documentBeingUpdated.doctype != "Product") {
            throw new Error("Wrong type of document");
        }

        // Create a ProductHistory document using the data from the document triggering the operation
        var priceDate = new Date().toISOString();
        var priceHistoryDoc =
        {
            id: documentBeingUpdated.productid + ":Price:" + priceDate,
            doctype: "ProductHistory",
            subcategory: documentBeingUpdated.subcategory,
            productid: documentBeingUpdated.productid,
            listprice: documentBeingUpdated.listprice,
            pricedate: priceDate
        }

        // Attempt to save the ProductHistory document
        var isAccepted = collection.createDocument(collection.getSelfLink(), priceHistoryDoc);

        // If the trigger is out of runtime, throw an error
        if (!isAccepted) {
            throw new Error('Unable to create price history document');
        }
    }
    ```

    > **Note:** For this exercise, the date is stored as a full ISO string, including the time down to the nearest milliscond. Previously, the document only stored the day, month, and year because it was assumed that a product would not change price more than once in a day. In this exercise, you are going to be performing a number of updates in quick succession, so just using the date without the time would result in multiple documents attempting to use the same document ID.

4. Save the trigger.

### Task 2: Compare the performance of using the trigger to performing the same operation in a client application

1. Start Visual Studio, and open the **PriceChangePerformanceTester** solution in the **E:\\Labfiles\\Lab04\\Starter\\Exercise04\\PriceChangePerformanceTester** folder.
2. Open the **Program.cs** file.
3. Locate the **DoWork** method in the **Worker** class. This method is very similar to that used by the **PriceChangeTester** app in exercise 3. The primary differences are that it takes an additional boolean parameter, **useTrigger**, that indicates whether the method should create the **ProductHIstory** document using the trigger you just created, or whether this task should be performed by the client.
4. Scroll down to the comment **Save the document back to the collection** and examine the code at this point:

    ```CSharp
    // Save the doc back to the collection
    double result = 0.0;
    if (useTrigger)
    {
        result = await SaveDocumentUsingTrigger(document);
    }
    else
    {
        result = await SaveDocumentWithoutUsingTrigger(document);
    }
    return result;
    ```

    This code calls either the **SaveDocumentUsingTrigger** or **SaveDocumentWithoutUsingTrigger** method, depending on the value of **useTrigger**.

5. Find the **SaveDocumentUsingTrigger** method. It looks like this:

    ```CSharp
    // Save the document using the trigger to add the price change history document
    // Return the request charge
    private async Task<double> SaveDocumentUsingTrigger(Document document)
    {
        var options = new RequestOptions
        {
            AccessCondition = new AccessCondition
            {
                Condition = document.ETag,
                Type = AccessConditionType.IfMatch
            },
            PreTriggerInclude = new List<string> { "CreatePriceHistoryDocument" }
        };

        var updateResponse = await client.UpsertDocumentAsync(
            UriFactory.CreateDocumentCollectionUri(this.database, this.collection), document, options);
        Console.WriteLine("Document updated using trigger");
        return updateResponse.RequestCharge;
    }
    ```

    This code is very similar to that used in the previous exercise, except that it runs the **CreatePriceHistoryDocument** trigger, and returns the request charge for performing this operation.

6. Find the **SaveDocumentWithoutUsingTrigger** method:

    ```CSharp
    // Save the document without using the trigger
    // Create and save the price change history document manually
    // Return the request charge
    private async Task<double> SaveDocumentWithoutUsingTrigger(Document document)
    {
        var options = new RequestOptions
        {
            AccessCondition = new AccessCondition
            {
                Condition = document.ETag,
                Type = AccessConditionType.IfMatch
            }
        };

        var updateResponse = await client.UpsertDocumentAsync(
            UriFactory.CreateDocumentCollectionUri(this.database, this.collection), document, options);
        Console.WriteLine("Document updated without using trigger");

        var historyDocument = new Document();
        var priceDate = DateTime.Now.ToString("yyyy-MM-dd HH:mm:ss.fffffff");
        var productID = document.GetPropertyValue<string>("productid");
        historyDocument.SetPropertyValue("id", $"{productID}:Price:{priceDate}");
        historyDocument.SetPropertyValue("doctype", "ProductHistory");
        historyDocument.SetPropertyValue("subcategory", document.GetPropertyValue<string>("subcategory"));
        historyDocument.SetPropertyValue("productid", productID);
        historyDocument.SetPropertyValue("listprice", document.GetPropertyValue<string>("listprice"));
        historyDocument.SetPropertyValue("pricedate", priceDate);

        var insertResponse = await client.CreateDocumentAsync(
            UriFactory.CreateDocumentCollectionUri(this.database, this.collection), historyDocument);
        Console.WriteLine("History document added");

        return updateResponse.RequestCharge + insertResponse.RequestCharge;
    }
    ```

    This code saves the **Product** document without firing the trigger. It then creates a **ProductHistory** document and inserts it into the database. The request charge for both operations is aggregated and returned.

7. Scroll up to the **Main** method of the **Program** class. This method prompts the user for a product id, and then iterates 100 times run the **DoWork** method both with and without invoking the trigger. The request charges are accumulated and displayed when the application completes. These figures should give you a comparison of the costs involved for performing the update using the trigger and by using code:

    ```CSharp
    static void Main(string[] args)
    {
        Console.WriteLine("Enter product ID");
        string id = Console.ReadLine();

        double requestChargeUsingTrigger = 0.0;
        double requestChargeWithoutUsingTrigger = 0.0;

        Worker worker = new Worker();
        for (int i = 0; i < 100; i++)
        {
            requestChargeUsingTrigger = worker.DoWork(id, true).Result;         // Use the trigger and get the response including the performance metrics
            requestChargeWithoutUsingTrigger = worker.DoWork(id, false).Result; // Don't use the trigger
        }

        Console.WriteLine($"Total request charge using trigger: {requestChargeUsingTrigger} RUs");
        Console.WriteLine($"Total request charge without using trigger: {requestChargeWithoutUsingTrigger} RUs");
        Console.WriteLine("Press Enter to finish");
        Console.ReadLine();
    }
    ```

8. Edit the **App.config** file, and add the URI and primary key of your Cosmos DB account where indicated in the **appSettings** section.
Build and run the application.
9. At the **Enter product ID** prompt, enter 730. This will update the price of product 740 200 times; 100 times using the trigger and 100 times using client code
10. When the app finishes note the request charges with and without using the trigger, and then press Enter to close it.
11. Run the app several more times, using products 731, 732, and 733. Each time note the request charges when the app completes.

    **Question**: What do you notice about the request charges for each run of the app?

12. Close the app, and then close Visual Studio.

### Task 3: Cleanup the lab environment

1. In Internet Explorer, in the left panel, click **Resource groups**.
2. In the **Resource groups** blade, right-click **20777Mod04**, and then click **Delete resource group**.
3. In the **Are you sure you want to delete "20777Mod04"?** blade, in the **TYPE THE RESOURCE GROUP NAME** box, type **20777Mod04**, and then click **Delete**.

---
 2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
