# Module 7: Implementing Stream Processing with Cosmos DB

- [Module 7: Implementing Stream Processing with Cosmos DB](#module-7-implementing-stream-processing-with-cosmos-db)
    - [Lab: Using Cosmos DB with Stream Processing](#lab-using-cosmos-db-with-stream-processing)
    - [Exercise 1: Monitoring stock levels](#exercise-1-monitoring-stock-levels)
        - [Task 1: Prepare the Environment](#task-1-prepare-the-environment)
        - [Task 2: Create a Cosmos DB collection for holding product sales statistics](#task-2-create-a-cosmos-db-collection-for-holding-product-sales-statistics)
        - [Task 3: Create an Azure Databricks account and cluster](#task-3-create-an-azure-databricks-account-and-cluster)
        - [Task 4: Create a Spark notebook for streaming orders data](#task-4-create-a-spark-notebook-for-streaming-orders-data)
        - [Task 5: Place orders and examine the streamed results](#task-5-place-orders-and-examine-the-streamed-results)
    - [Exercise 2: Handling orders](#exercise-2-handling-orders)
        - [Task 1: Create Azure ServiceBus Queues](#task-1-create-azure-servicebus-queues)
        - [Task 2: Create an Azure Functions App account](#task-2-create-an-azure-functions-app-account)
        - [Task 3: Create the HandleOrders function](#task-3-create-the-handleorders-function)
        - [Task 4: Test the HandleOrders function](#task-4-test-the-handleorders-function)
        - [Task 5: Observe the query overhead of using the change feed](#task-5-observe-the-query-overhead-of-using-the-change-feed)
    - [Exercise 3: Displaying rolling revenue for a given time period](#exercise-3-displaying-rolling-revenue-for-a-given-time-period)
        - [Task 1: Create a Databricks notebook for calculating rolling revenue](#task-1-create-a-databricks-notebook-for-calculating-rolling-revenue)
        - [Task 2: Place orders and observe the rolling revenue](#task-2-place-orders-and-observe-the-rolling-revenue)
    - [Exercise 4: Handling product deliveries](#exercise-4-handling-product-deliveries)
        - [Task 1: Create an Azure IoT Hub](#task-1-create-an-azure-iot-hub)
        - [Task 2: Create an Azure Stream Analytics Job](#task-2-create-an-azure-stream-analytics-job)
        - [Task 3: Create a Cosmos DB collection for holding the details of product deliveries](#task-3-create-a-cosmos-db-collection-for-holding-the-details-of-product-deliveries)
        - [Task 4: Connect the IoT Hub, Stream Analytics Job, and Cosmos DB Database](#task-4-connect-the-iot-hub-stream-analytics-job-and-cosmos-db-database)
        - [Task 5: Create the ProcessDeliveries function](#task-5-create-the-processdeliveries-function)
        - [Task 6: Simulate deliveries arriving at the warehouse](#task-6-simulate-deliveries-arriving-at-the-warehouse)
        - [Task 7: Cleanup the lab environment](#task-7-cleanup-the-lab-environment)

## Lab: Using Cosmos DB with Stream Processing

## Exercise 1: Monitoring stock levels

### Task 1: Prepare the Environment

1. Ensure that the **MT17B-WS2016-NAT** and **20777A-LON-DEV** virtual machines are running, and then log on to **20777A-LON-DEV** as **LON-DEV\\Administrator** with the password **Pa55w.rd**.
2. In File Explorer, navigate to **E:\\Labfiles\\Lab07\\Starter**, right-click **full-cosmos-setup.ps1.txt**, and then click **Edit**.
3. In Notepad, edit **20777-mod7-sql-&lt;your initials&gt;&lt;day&gt;** and change the  **&lt;your initials&gt;&lt;day&gt;** to your initials and a number between 1 and 31, for example **20777-mod7-sql-pjs14**.
4. Edit **20777blobmod7&lt;your initials&gt;** and change the **&lt;your initials and day&gt;** to your initials and a number between 1 and 31, for example, **20777blobmod7pjs14**.
5. On line 19, change the **resourceGroupLocation** variable to specify the name of your nearest location.
6. On the **File** menu, click **Save**, and then close Notepad.
7. In File Explorer, right-click **Setup.cmd**, and then click **Run as administrator**.
8. When prompted enter your Azure credentials.
9. When the script completes, press any key to close the command window.
10. On the toolbar, click **Internet Explorer**.
11. In Internet Explorer, go to **http://portal.azure.com**, and sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
12. In the Azure portal, in the left panel, click **All resources**, and then click **20777-mod7-sql-&lt;your initials and day&gt;**.
13. On the **20777-mod7-sql-&lt;your initials and day&gt;** blade, under **Settings**, click **Keys**.
14. Make a note of the **URI**, and **PRIMARY KEY** values.
15. In the left panel, click **All resources**, and then click **20777blobmod7&lt;your initials and day&gt;**.
16. On the **20777blobmod7&lt;your initials and day&gt;** blade, under **Settings**, click **Access keys**.
17. Make a note of the **Connection string** under **key1**.
18. On the Windows Start menu, click **Visual Studio 2017**.
19. On the **File** menu, point to **Open**, and then click **Project/Solution**.
20. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab07\\Starter\\MigrateProductData**, click **MigrateProductData.sln**, and then click **Open**.
21. In Solution Explorer, click **App.config**.
22. On the **App.config** tab, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** you noted earlier for the **20777-mod7-sql-&lt;your initials and day&gt;** Cosmos DB account.
23. In the **BlobStorageConnectionString** setting, replace **\~CONNECTION STRING\~** with the **key1 Connection string** you noted earlier for the **20777blobmod7&lt;your initials and day&gt;** blob storage account.
24. Press F5 to build and run the application. 
25. In the console window, wait until the application has completed, and the close the application.
26. Close Visual Studio.

### Task 2: Create a Cosmos DB collection for holding product sales statistics

1. In Internet Explorer, in the Azure portal, in the left panel, click **All resources**, and then click **20777-mod7-sql-&lt;your initials and day&gt;**.
2. On the **20777-mod7-sql-&lt;your initials and day&gt;** blade, click **Data Explorer**.
3. In the **SQL API** pane, right-click **Adventure-Works**, and then click **New Collection**.
4. On the **Add Collection** blade, in the **Collection Id** box, type **ProductSales**.
5. Under **Storage capacity**, click **Unlimited**.
6. In the **Partition key** box, type **/productnumber**, and then click **OK**.

### Task 3: Create an Azure Databricks account and cluster

1. In the Azure portal, in the left panel, click **+ Create a resource**, in the search box, type **Azure Databricks**, and then press Enter.
2. On the **Everything** blade, click **Azure Databricks**, and then click **Create**.
3. On the **Azure Databricks Service** blade, in the **Workspace name** box, type **20777a-databricks-&lt;your name&gt;-&lt;the day&gt;**.
4. Under **Resource group**, click **Use existing**, and then in the drop-down list, click **20777Mod07**.
5. In the **Location** drop-down list, click **West US 2**.

    > **Note:** Currently, not all regions support the full range of VMs used by Azure Databricks to host Spark clusters, **West US 2** does.

6. In the **Pricing Tier** drop-down list, click **Trial (Premium - 14-Days Free DBUs)**, and then click **Create**.
7. Wait while the service is deployed.
8. In the left panel, click **All resources**, and then click **20777a-databricks-&lt;your name&gt;-&lt;the day&gt;**.
9. On the **20777a-databricks-&lt;your name&gt;-&lt;the day&gt;** blade, click **Launch Workspace**.
10. On the **Azure Databricks** page, under **Common Tasks**, click **New Cluster**.
11. On the **New Cluster** page, in the **Cluster Name** box, type **&lt;your name&gt;-cluster**.
12. In the **Max Workers** box, type **4**.
13. Leave all other settings at their default values, and then click **Create Cluster**.
14. Wait while the cluster is created and started. Verify that its **State** is set to **Running** before continuing.

### Task 4: Create a Spark notebook for streaming orders data

> **Note:** The Scala code used in this task can be found in the file **E:\\Labfiles\\Lab07\\Solution\\Exercise01\\monitorstocklevels.txt**.

1. In the left toolbar, click **Azure Databricks**.
2. Under **Common Tasks**, click **Import Library**.
3. On the **New Library** page, in the **Source** drop-down list, click **Upload Java/Scala JAR**.
4. In the **Library Name** box, type **Cosmos DB Connector**.
5. Click the **Drop library JAR here to upload** text.
6. In the **JAR File** box, go to the **E:\Labfiles\Lab07\Starter\Exercise01** folder, click **azure-cosmosdb-spark_2.3.0_2.11-1.2.0-uber.jar**, and then click **Open**.
7. When the JAR file has been uploaded, click **Create Library**.
8. On the **Cosms DB Connector** page, in the **&lt;your name&gt;-cluster** row, select the **Attach** check box, and wait until the status of the library changes to **Attached**.
9. In the toolbar on the left of the blade, click **Azure Databricks**.
10. Under **Common Tasks**, click **New Notebook**.
11. In the **Create Notebook** dialog box, in the **Name** box, type **monitorstocklevels**.
12. In the **Language** drop-down list, click **Scala**.
13. In the **Cluster** drop-down list, click **&lt;your name&gt;-cluster**, and then click **Create**.
14. In the first cell of the notebook, enter the following code.

    ```Scala
    // Import libraries required for streaming, and for the Cosmos DB connector

    import org.apache.spark.SparkConf
    import org.apache.spark.SparkContext
    import com.microsoft.azure.cosmosdb.spark.config.Config
    import com.microsoft.azure.cosmosdb.spark.schema._
    import com.microsoft.azure.cosmosdb.spark.streaming._
    import org.apache.spark.sql.functions._
    import java.sql.Timestamp
    ```

15. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
16. In the new cell, type the following code. Replace **--URI--** with the **URI**, and replace **--KEY--** with the **PRIMARY KEY** values that you noted earlier for the **20777a-mod7-sql-&lt;your name&gt;-&lt;the day&gt;** Cosmos DB account.

    ``` Scala
    // Configure the connection to the Cosmos DB database

    val databaseReadConfig = Map(
        "endpoint" -> "--URI--",
        "masterkey" -> "--KEY--",
        "database" -> "Adventure-Works",
        "collection" -> "Data",
        "readchangefeed" -> "true",
        "changefeedqueryname" -> "ordersquery",
        "changefeedstartfromthebeginning" -> "false",
        "samplingratio" -> "1.0",
        "schema_samplesize" -> "1000",
        "changefeedcheckpointlocation" -> "/checkpointlocation"
    )
    ```

17. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
18. In the new cell, type the following code:

    ```Scala
    // Specify that spark should stream data from the change feed

    val changeFeedStream = spark
        .readStream
        .format(classOf[CosmosDBSourceProvider].getName)
        .options(databaseReadConfig)
        .load()
    ```

19. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
20. In the new cell, type the following code:

    ```Scala
    // The change feed contains documents of many types
    // Filter out all documents that are not new orders

    val ordersStream = changeFeedStream.filter("doctype == 'ShoppingCartOrder' AND isshoppingcartororder == 'Order' AND orderstatus == 'In progress'")
    ```
21. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
22. In the new cell, type the following code:

    ```Scala
    // Isolate the orderitems data, an array containing the number of items ordered for each product in the order, from the rest of the data

    val orderitemsFrame = ordersStream.select(explode($"orderitems").as("orderitems"))
    ```

23. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
24. In the new cell, type the following code:

    ```Scala
    // Add the current timestamp to the data frame.
    // This will be used for time watermarking and windowing

    val orderitemsFrameWithTimestamp = orderitemsFrame.withColumn("now", current_timestamp)
    ```

25. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
26. In the new cell, type the following code:

    ```Scala
    // Calculate the number sold and number placed on backorder for products in all orders placed in the last 30 seconds

    val productsSoldFrame = orderitemsFrameWithTimestamp
        .withWatermark("now", "15 seconds")
        .groupBy($"orderitems.productname", $"orderitems.productnumber", window($"now", "30 seconds").as("timeslot"))
        .agg(sum($"orderitems.numberincartorordered").as("numberordered"), sum($"orderitems.numberonbackorder").as("numberonbackorder"))
    ```

27. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
28. In the new cell, type the following code. Replace **--URI--** with the **URI**, and replace **--KEY--** with the **PRIMARY KEY** values that you noted earlier for the **20777a-mod7-sql-&lt;your name&gt;-&lt;the day&gt;** Cosmos DB account.

    ```Scala
    // Stream the results back to Cosmos DB

    val databaseWriteConfig = Map(
        "endpoint" -> "--URI--",
        "masterkey" -> "--KEY--",
        "database" -> "Adventure-Works",
        "collection" -> "ProductSales",
        "checkpointLocation" -> "/checkpointwritelocation" // Note the capital "L" in "checkpointLocation"
    )

    val results = productsSoldFrame
        .select($"productnumber", $"productname", $"numberordered", $"numberonbackorder", date_format($"timeslot.end", "HH:mm:ss").as("timeslot"))
        .writeStream
        .format(classOf[CosmosDBSinkProvider].getName)
        .outputMode("append")
        .options(databaseWriteConfig)
        .start()
    ```

### Task 5: Place orders and examine the streamed results

1. On the Windows Start menu, click **Visual Studio 2017**.
2. On the **File** menu, point to **Open**, and then click **Project/Solution**.
3. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab07\\Starter\\Exercise01\\PlaceOrders**, and then double-click **PlaceOrders.sln**. This app simulates customers placing orders.
4. In Solution Explorer, click **App.config**.
5. On the **App.config** tab, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** values that you noted earlier for the **20777a-mod7-sql-&lt;your name&gt;-&lt;the day&gt;** Cosmos DB account.
6. Press F5 to build and run the application. You will see messages indicating that orders are being created and added to the database.
7. Allow the app to run for a minute, and then return to the Databricks notebook.
8. In the toolbar at the top of the notebook, click **Run All**. Wait for the final cell in the notebook to indicate that it has started streaming data; the cell should run continuously, and you will see a **Last updated** message appear at regular intervals.

    > **Note**: The cell that starts streaming the data from the change feed maight report the error **Failed to load class "org.slf4j.impl.StaticLoggerBinder"**. You can ignore this error.

9.  Switch to the Azure portal.
10. In Internet Explorer, in the left panel, click **All resources**, and then click **20777-mod7-sql-&lt;your initials and day&gt;**.
11. On the **20777-mod7-sql-&lt;your initials and day&gt;** blade, click **Data Explorer**, expand **Adventure-Works**, expand **ProductSales**, and then click **Documents**.
12. Click any document in the collection. It should contain data similar to that shown below. The **timeslot** field indicates the end of the 30 second period during which the data was captured. The **numberordered** value shows how many items of the specified product were ordered in that interval, and the **numberonbackorder** value indicates how many items were placed on backorder because there is insufficient stock in place to satisfy orders.

    ```JSON
    {
        "productnumber": "FR-M94B-42",
        "timeslot": "11:32:00",
        "productname": "HL Mountain Frame - Black, 42",
        "numberordered": 1,
        "numberonbackorder": 2,
        "id": "65ec316e-b973-49eb-aed8-ff4df0760872",
        "_rid": "T3ILALy68wAKAAAAAAAAAA==",
        "_self": "dbs/T3ILAA==/colls/T3ILALy68wA=/docs/T3ILALy68wAKAAAAAAAAAA==/",
        "_etag": "\"0200812d-0000-0000-0000-5b7bf85e0000\"",
        "_attachments": "attachments/",
        "_ts": 1534851166
    }
    ```

13. In the **SQL API** pane, click **New SQL Query**.
14. On the **Query 1** tab, enter the following query. Replace **&lt;Product Number&gt;** with the product number from the document that you just browsed, for example **FR-M94B-42**.

    ```SQL
    SELECT * FROM c WHERE c.productnumber = "<Product Number>"
    ```

15. Click **Execute Query**. You will see sales summary documents for each 30 second period during which orders were placed for the product. If any periods are missing, then no orders were received for the item.
16. Note the number of documents returned by the query, wait for a minute, and then click **Execute Query**. If more orders have been placed, then more documents should be created.
17. In Visual Studio 2017, on the **Debug** menu, click **Stop Debugging**, and then close Visual Studio.
18. In Internet Explorer, in the **monitorstocklevels** notebook, in the toolbar at the top of the notebook, click **Stop Execution**.
19. Close the **Azure Databricks** portal.
20. In the **Windows Internet Explorer** dialog box, click **Leave this page**.

## Exercise 2: Handling orders

### Task 1: Create Azure ServiceBus Queues

1. In the Azure portal, in the left panel, click **+ Create a resource**.
2. On the **New** blade, in the search box, type **Azure ServiceBus**, and then press Enter.
3. On the **Everything** blade, click **Service Bus**, and then click **Create**.
4. On the **Create namespace** blade, in the **Name** box, type **sb-20777-&lt;your name&gt;-&lt;the day&gt;**, for example, **sb-20777-john-31**.
5. In the **Pricing tier** drop-down list box, click **Basic**.
6. Under **Resource group**, in the drop-down list, click **20777Mod07**.
7. In the **Location** box, select a location near you, and then click **Create**.
8. In the Azure portal, in the left panel, click **All resources**, and then click **sb-20777-&lt;your name&gt;-&lt;the day&gt;**.
9. On the **sb-20777-&lt;your name&gt;-&lt;the day&gt;** blade, click **+ Queue**.
10. On the **Create queue** blade, in the **Name** box, type **customerordersqueue**, leave the remaining fields at their default values, and then click **Create**.
11. On the **sb-20777-&lt;your name&gt;-&lt;the day&gt;** blade, click **+ Queue**.
12. On the **Create queue** blade, in the **Name** box, type **warehousequeue**, leave the remaining fields at their default values, and then click **Create**.
13. On the **sb-20777-&lt;your name&gt;-&lt;the day&gt;** blade, under **Settings**, click **Shared access policies**.
14. On the **Shared access policies** blade, click the **RootManageSharedAccessKey** policy.
15. Make a note of the **Primary Connection String** value.

### Task 2: Create an Azure Functions App account

1. In the left panel, click **+ Create a resource**.
2. In the **New** blade, in the search box, type **Function App**, and then press Enter.
3. On the **Everything** blade, click **Function App**, and then click **Create**.
4. On the **Function App** blade, in the **App name** box, type **20777-func-&lt;your name&gt;-&lt;the day&gt;**, for example, **20777-func-john-31**.
5. Under **Resource Group**, click **Use existing**, then in the drop-down list, click **20777Mod07**.
6. In the **Location** box, select a location near you; you should use the same location that you used for your Cosmos DB account if it appears in the list.
7. Under **Application Insights**, click **Off**.
8. Leave all other settings at their default values, and then click **Create**.

### Task 3: Create the HandleOrders function

> **Note:** The complete code for the **HandleOrders** function is available in the file **E:\\Labfiles\\Lab07\\Solution\\Exercise02\\HandleOrders.txt**.

1. In the left panel, click **All resources**, and then click **20777-func-&lt;your name&gt;-&lt;the day&gt;**.
2. On the **20777-func-&lt;your name&gt;-&lt;the day&gt;** blade, click **Functions**, and then click **+ New function**.
3. On the **Choose a template below or go to the quickstart** blade, click **Cosmos DB trigger**. 

    > **Note**: If the **Extensions not Installed** pane appears, click **Install** to install the template dependencies required to create Cosmos DB triggers, and then click **Continue** when installation is complete.

4. In the **Name** box, type **HandleOrders**.
5. Under **Azure Cosmos DB trigger**, click **new**.
6. In the **Connection** dialog box, click **Azure Cosmos DB account**.
7. In the **Subscription** drop-down list, select your own subscription.
8. In the **Database Account** drop-down list, click **20777-mod7-sql-&lt;your initials and day&gt;**, and then click **Select**.
9. On the **New Function** blade, in the **Collection name** box, type **Data**.
10. Select the **Create lease collection if it does not exist** check box.
11. In the **Database name** box, type **Adventure-Works**.
12. Leave **Collection name for leases** set to **leases**, and then click **Create**.
13. On the **run.csx** blade, type the following code to add references to the **Microsoft.ServiceBus** and **Newtonsoft.Json** assemblies at the top of the file, after the reference to the **Microsoft.Azure.Documents.Client** assembly:

    ```CSharp
    #r "Microsoft.ServiceBus"
    #r "Newtonsoft.Json"
    ```

14. Add the following **Using** directives to the code, that bring the **Microsoft.ServiceBus.Messaging** and the **Newtonsoft.Json** namespaces into scope, as shown below:

    ```CSharp
    using Microsoft.ServiceBus.Messaging;
    using Newtonsoft.Json;
    ```

15. After the closing brace of the **Run** function, add the following class definitions to the file:

    ```CSharp
    // Base class for all root document types
    public abstract class DocumentType
    {
        [JsonProperty("doctype")]
        public string DocType { get; set; }

        [JsonProperty("ttl")]
        public int TimeToLive { get; set; } = -1;

        public DocumentType()
        {
            this.DocType = this.GetType().Name;
        }
    }

    public class OrderItem
    {
        // Define properties
        [JsonProperty("productnumber")]
        public string ProductNumber { get; set; }

        [JsonProperty("productname")]
        public string ProductName { get; set; }

        [JsonProperty("productid")]
        public string ProductID { get; set; }

        [JsonProperty("subcategory")]
        public string Subcategory { get; set; }

        [JsonProperty("numberincartorordered")]
        public int NumberInCartOrOrdered { get; set; }

        [JsonProperty("numberonbackorder")]
        public int NumberOnBackorder { get; set; }

        [JsonProperty("backorderreference")]
        public string BackorderReference { get; set; }

        [JsonProperty("unitcost")]
        public decimal UnitCost { get; set; }

        [JsonProperty("lineitemtotalcost")]
        public decimal LineItemTotalCost { get; set; }
    }

    public class ShoppingCartOrder : DocumentType
    {
        // Define properties
        [JsonProperty("id")]
        public string ShoppingCartOrderID { get; set; }

        [JsonProperty("partitionkey")]
        public string CustomerID { get; set; }

        [JsonProperty("isshoppingcartororder")]
        public string IsShoppingCartOrOrder { get; set; }

        [JsonProperty("orderitems")]
        public List<OrderItem> OrderItems { get; set; }

        [JsonProperty("numberofitems")]
        public int NumberOfItems { get; set; }

        [JsonProperty("itemscost")]
        public decimal ItemsCost { get; set; }

        [JsonProperty("customerdiscountrate")]
        public int CustomerDiscountRate { get; set; }

        [JsonProperty("totalcost")]
        public decimal TotalCost { get; set; }

        [JsonProperty("dateplaced")]
        public long DatePlaced { get; set; }

        [JsonProperty("orderstatus")]
        public string OrderStatus { get; set; } // "In progress", "Delivered", "Cancelled"

        [JsonProperty("lastupdated")]
        public long LastUpdated { get; set; }
    }
    ```

16. Modify the definition of the **Run** method to indicate that it supports asynchronous operations.

    ```CSharp
    public static async Task Run(IReadOnlyList<Document> documents, ILogger log)
    ```

17. Remove the following two statements from the body of the function:

    ```CSharp
    log.LogInformation("Documents modified " + documents.Count);
    log.LogInformation("First document Id " + documents[0].Id);
    ```

18. Inside the **if** statement, add the following two lines of code that create client connections for each of the ServiceBus message queues that you created earlier. Replace the **&lt;Connection String&gt;** with the **Primary Connection String** value that you noted earlier for the **sb-20777-&lt;your name&gt;-&lt;the day&gt;** Service Bus.

    ```CSharp
    // Create clients for connecting to the Azure ServiceBus queues
    QueueClient customerOrdersQueueClient = QueueClient.CreateFromConnectionString("<Connection String>", "customerordersqueue");
    QueueClient warehouseQueueClient = QueueClient.CreateFromConnectionString("<Connection String>", "warehousequeue");
    ```
19. Still inside the **if** statement, immediately after the code you have just entered, add the following statements.

    ```CSharp
    // Iterate through the change feed
    foreach (Document doc in documents)
    {

    }
    ```

20. Inside the **foreach** statement, add the following code:

    ```CSharp
    // The change feed can contain documents of many different types (customers, products, orders, etc)
    // We are only interested in new orders ("In progress"), so filter out the others types of documents and orders ("Delivered", "Cancelled")
    ShoppingCartOrder orderDoc = JsonConvert.DeserializeObject<ShoppingCartOrder>(doc.ToString());
    if (orderDoc.DocType == "ShoppingCartOrder" && orderDoc.IsShoppingCartOrOrder == "Order" && orderDoc.OrderStatus == "In progress")
    {

    }
    ```

21. Inside the nested **if** statement you added in the previous step, add the following code:

    ```CSharp
    // The doc is for an order
    // Alert the warehouse with the details of the items in the order
    foreach (OrderItem item in orderDoc.OrderItems)
    {
        log.LogInformation($"New order for {item.NumberInCartOrOrdered} of {item.ProductNumber} ({item.ProductName})");
        BrokeredMessage orderItemMessage = new BrokeredMessage(JsonConvert.SerializeObject(item));
        await warehouseQueueClient.SendAsync(orderItemMessage);
    }

    // Send a message to the customer orders department containing the details of the order
    log.LogInformation($"New order, {orderDoc.ShoppingCartOrderID}, for customer {orderDoc.CustomerID}. Value is {orderDoc.TotalCost}");
    BrokeredMessage orderMessage = new BrokeredMessage(JsonConvert.SerializeObject(orderDoc));
    await customerOrdersQueueClient.SendAsync(orderMessage);
    ```

22. Before the closing brace of the outermost **if** statement, add the following code:

    ```CSharp
    await customerOrdersQueueClient.CloseAsync();
    await warehouseQueueClient.CloseAsync();
    ```
23. Click **Save**.

### Task 4: Test the HandleOrders function

1. On the **run.csx** blade, click **Logs**.
2. On the Windows Start menu, click **Visual Studio 2017**.
3. On the **File** menu, point to **Open**, and then click **Project/Solution**.
4. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab07\\Starter\\Exercise02\\Adventure-Works**, and then double-click **Adventure-Works.sln**.
5.  In Solution Explorer, click **Web.config**.
6.  On the **Web.config** tab, in the **appSettings** section, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** values that you noted earlier for the **20777-mod7-sql-&lt;your initials and day&gt;** Cosmos DB account.
7.  Press F5 to build and run the application.
8.  In the web app, click **Search By Category**. For the first item returned by the search, click **Details**, and then click **Add to Shopping Cart**.
9.  On the **Login** page, in the **Customer ID** box, type **1**, and then click **Login**.
10. In the **Item added to shopping cart** dialog box, click **OK**.
11. Do not click **Place Order** just yet.
12. On the **Azure portal** tab, showing the log trace for the **HandleOrders** function, verify that it displays messages similar to these:

    ```Text
    2018-08-19T16:30:20.375 [Info] Function started (Id=6b544ed7-6f3e-4cf8-9f8a-da1ba7fe4e97)
    2018-08-19T16:30:21.252 [Info] Function completed (Success, Id=6b544ed7-6f3e-4cf8-9f8a-da1ba7fe4e97, Duration=876ms)
    ```
13. On the **Adventure-Works** tab, click **Place Order**.
14. In the **Order placed** dialog box, click **OK**.
15. On the **Azure portal** tab, in the log for the Azure function, you should now see messages similar to these included in the output:

    ```Text
    2018-08-19T17:01:10.512 [Verbose] New order for 1 of BK-R93R-48 (Road-150 Red, 48)
    2018-08-19T17:01:11.247 [Verbose] New order, af8eae4a-5ba9-455d-9935-a9aa61d65134, for customer 7. Value is 3542.4873
    ```
16. On the Start menu, click **Visual Studio 2017** to start another instance of Visual Studio 2017.
17. On the **File** menu, point to **Open**, and then click **Project/Solution**.
18. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab07\\Starter\\Exercise02\\CustomerOrders**, and then double-click **CustomerOrders.sln**. This app monitors the customerordersqueue ServiceBus queue.
19. In Solution Explorer, click **App.config**.
20. On the **App.config** tab, in the **appSettings** section, replace **\~CONNECTION STRING\~** with the **Primary Connection String** value that you noted earlier for the **sb-20777-&lt;your name&gt;-&lt;the day&gt;** Service Bus.
21. Press F5 to build and run the application. You should see a message appear that describes the order you just placed using the Adventure-Works web site (if you placed more than one order, you will see more than one message). This message was posted by the **HandleOrders** Azure Function App.
22. Leave the app running.
23. On the Start menu, click **Visual Studio 2017** to start another instance of Visual Studio 2017.
24. On the **File** menu, point to **Open**, and then click **Project/Solution**.
25. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab07\\Starter\\Exercise02\\Warehouse**, and then double-click **Warehouse.sln**. This app monitors the warehousequeue ServiceBus queue.
26. In Solution Explorer, click **App.config**.
27. On the **App.config** tab, in the **appSettings** section, replace **\~CONNECTION STRING\~** with the **Primary Connection String** value that you noted earlier for the **sb-20777-&lt;your name&gt;-&lt;the day&gt;** Service Bus.
28. Press F5 to build and run the application. You should see a messages appear that describe the items in the order you just placed. These messages were also posted by the **HandleOrders** Azure Function App.
29. On the **Adventure-Works** tab, place a few more orders. Notice how the details of the orders are processed by the HandleOrders function app, and the results displayed by the CustomerOrders and Warehouse apps.
30. When you have finished, close the Aventure-Works tab, but leave the **CustomerOrders**, and **Warehouse** apps running.

### Task 5: Observe the query overhead of using the change feed

1. In the left panel, click **All resources**, and then click **20777-mod7-sql-&lt;your initials and day&gt;**.
2. On the **20777-mod7-sql-&lt;your initials and day&gt;** blade, under **MONITORING**, click **Metrics**.
3. On the **Metrics** blade, on the **Throughput** tab, in the **Database(s)** drop-down list, click **Adventure-Works**.
4. In the **Collection(s)** drop-down list, click **Data**.
5. On the Start menu, click **Visual Studio 2017** to start another instance of Visual Studio 2017.
6. On the **File** menu, point to **Open**, and then click **Project/Solution**.
7. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab07\\Starter\\Exercise02\\PlaceOrders**, and then double-click **PlaceOrders.sln**.
8. In Solution Explorer, click **App.config**.
9.  On the **App.config** tab, in the **appSettings** section, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** values that you noted earlier for the **20777-mod7-sql-&lt;your initials and day&gt;** Cosmos DB account.
10. Press F5 to build and run the application. You will see messages indicating that orders are being created and added to the database.
11. Switch to the window displaying the ouput of the **CustomerOrders** app, and verify that you see new orders being processed.
12. Wait for 5 or 10 minutes while the PlaceOrders generates orders.

    > **Note:** The CustomerOrders app might display an exception ocassionally when it finds an unexpected character appearing in a message retrieved from the queue. You can ignore these exceptions.

13. Leave the PlaceOrders app running, and return to the Azure portal displaying the throughput for the Cosmos DB database.
14. Click **Refresh** to see the most recent statistics. Note that the updated statistics will not appear immediately, so you might need to wait for a couple more minutes and then click **Refresh** again.
15. Make a note of the number of requests aggregated over a 1 minute interval.
16. In the left panel, click **All resources**, and then click **20777-func-&lt;your initials and day&gt;**.
17. In the **Message from webpage** dialog box, click **OK**.
18. On the **20777-func-&lt;your initials and day&gt;** blade, click **Stop** to halt the function app.
19. In the **Message from webpage** dialog box, click **OK**.
20. In the left panel, click **All resources**, and then click **20777-mod7-sql-&lt;your initials and day&gt;**.
21. On the **20777-mod7-sql-&lt;your initials and day&gt;** blade, under **MONITORING**, click **Metrics**.
22. On the **Metrics** blade, on the **Throughput** tab, in the **Database(s)** drop-down list, click **Adventure-Works**.
23. In the **Collection(s)** drop-down list, click **Data**.
24. Wait for a few minutes, and then periodically click **Refresh**. You should see that the number of requests aggregated over a 1 minute interval drops to less than half the previous value (the **PlaceOrders** app should still be running, although the **CustomerOrders** and **Warehouse** apps will appear to have stalled as no more messages are appearing on the ServiceBus queues). Although the function app was not performing any explicit queries, listening to the change feed adds significantly to the workload on the database. You should bear this overhead in mind when building your own applications that use the change feed of a collection.
25. Stop the **Warehouse**, **CustomerOrders**, and **PlaceOrders** apps, and close all instances of Visual Studio.

## Exercise 3: Displaying rolling revenue for a given time period

### Task 1: Create a Databricks notebook for calculating rolling revenue

> **Note:** The Scala code used in this task can be found in the file **E:\\Labfiles\\Lab07\\Solution\\Exercise03\\rollingrevenue.txt**.

1. In teh Azure portal, in the left panel, click **All resources**, and then click **20777a-databricks-&lt;your name&gt;-&lt;the day&gt;**.
2. On the **20777a-databricks-&lt;your name&gt;-&lt;the day&gt;** blade, click **Launch Workspace**.
3. On the **Azure Databricks** page, under **Common Tasks**, click **New Notebook**.
4. In the **Create Notebook** dialog box, in the **Name** box, type **rollingrevenue**.
5. In the **Language** drop-down list, click **Scala**.
6. In the **Cluster** drop-down list, click **&lt;your name&gt;-cluster**, and then click **Create**.
7. In the first cell of the notebook, type the following code:

    > **Note:** The code for the first few cells in the notebook is the same as the earlier exercise. This is because the notebook streams data from the change feed of the same collection.

    ```Scala
    // Import libraries required for streaming, and for the Cosmos DB connector

    import org.apache.spark.SparkConf
    import org.apache.spark.SparkContext
    import com.microsoft.azure.cosmosdb.spark.config.Config
    import com.microsoft.azure.cosmosdb.spark.schema._
    import com.microsoft.azure.cosmosdb.spark.streaming._
    import org.apache.spark.sql.functions._
    import java.sql.Timestamp
    ```

9. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
10. In the new cell, type the following code. replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** values that you noted earlier for the **20777-mod7-sql-&lt;your initials and day&gt;** Cosmos DB account.

    ``` Scala
    // Configure the connection to the Cosmos DB database

    val databaseReadConfig = Map(
        "endpoint" -> "--URI--",
        "masterkey" -> "--KEY--",
        "database" -> "Adventure-Works",
        "collection" -> "Data",
        "readchangefeed" -> "true",
        "changefeedqueryname" -> "ordersquery",
        "changefeedstartfromthebeginning" -> "false",
        "samplingratio" -> "1.0",
        "schema_samplesize" -> "1000",
        "changefeedcheckpointlocation" -> "/checkpointlocation2"
    )
    ```

11. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
12. In the new cell, type the following code:

    ```Scala
    // Specify that spark should stream data from the change feed

    val changeFeedStream = spark
        .readStream
        .format(classOf[CosmosDBSourceProvider].getName)
        .options(databaseReadConfig)
        .load()
    ```

13. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
14. In the new cell, type the following code:

    ```Scala
    // Filter out all documents that are not new orders

    val ordersStream = changeFeedStream.filter("doctype == 'ShoppingCartOrder' AND isshoppingcartororder == 'Order' AND orderstatus == 'In progress'")
    ```

15. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
16. In the new cell, type the following code:

    ```Scala
    // Extract the revenue details from the dataframe, and include the current time

    val revenueDetailsFrame = ordersStream
        .select($"totalcost", $"itemscost")
        .withColumn("now", current_timestamp)
    ```

17. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
18. In the new cell, type the following code:

    ```Scala
    // Calculate the rolling revenue for the 5 minutes, with a sliding window of 1 minute

    val revenuePerMinute = revenueDetailsFrame
        .withWatermark("now", "30 seconds")
        .groupBy(window($"now", "5 minutes", "1 minute").as("time"))
        .agg(sum($"totalcost").as("revenueperminute"), sum($"itemscost").as("revenueperminutebeforediscount"))
    ```

19. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
20. In the new cell, type the following code:

    ```Scala
    // Capture the rolliong revenue to a table in memory

    val query = revenuePerMinute
        .writeStream
        .format("memory")        // memory = store data in-memory in a table (only do this if the results of the streaming aggregation are low-volume)
        .queryName("revenue")    // revenue = name of the in-memory table
        .outputMode("append")  
        .start()
    ```

21. In the toolbar at the top right-hand side of the cell, on the **Edit Menu** menu, click **Add Cell Below**.
22. In the new cell, type the following code:

    ```Scala
    %sql
    select revenueperminute, revenueperminutebeforediscount, revenueperminutebeforediscount - revenueperminute as discounted, time.end as time 
    from revenue
    ```

### Task 2: Place orders and observe the rolling revenue

1. On the Start menu, click **Visual Studio 2017**.
2. On the **File** menu, point to **Open**, and then click **Project/Solution**.
3. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab07\\Starter\\Exercise03\\PlaceOrders**, and then double-click **PlaceOrders.sln**.
4. In Solution Explorer, click **App.config**.
5. On the **App.config** tab, replace **\~URI\~** with the **URI**, and replace **\~KEY\~** with the **PRIMARY KEY** values that you noted earlier for the **20777-mod7-sql-&lt;your initials and day&gt;** Cosmos DB account.
6. Press F5 to build and run the application. You will initially see messages indicating that old orders are being deleted; this is simply to clear data out of the database.
7. At the prompt, press Enter to generate new orders. You will see messages indicating that new orders are being created.
8. Allow the app to run for a minute, and then return to the Databricks notebook  (leave the app running).
9. In the toolbar at the top of the notebook, click **Run All**. Wait for the penultimate cell in the notebook to indicate that it has started streaming data; the cell should run continuously, and you will see a **Last updated** message appear at regular intervals.
10. Allow the notebook to run for another minute, and then in the toolbar at the top right-hand side of the final cell in the notebook, on the **Run Cell** menu, click **Run Cell**. A table displaying data for the last few minutes (1 row per minute) should appear. The figures in the **revenueperminute** and **revenueperminutebeforediscount** columns show the rolling revenue for the 5 minutes that end in the **time** column.
11. Below the table, click the graph drop-down arrow, and then click **Line (deprecated)**.
12. Click **Plot Options**.
13. In the **Customize Plot** dialog box, delete all columns from the **Keys**, **Series groupings**, and **Values** boxes.
14. In the **All fields** box, click the **time** column and drag it into the **Keys** box.
15. In the **All fields** box, click the **revenueperminute** column and drag it into the **Values** box.
16. In the **All fields** box, click the **revenueperminutebeforediscount** column and drag it into the **Values** box.
17. In the **All fields** box, click the **discounted** column and drag it into the **Values** box.
18. Click **Apply**. The table in the notebook should be replaced with a graph depicting the rolling revenue values.
19. In the toolbar at the top right-hand side of the cell, on the **Run Cell** menu, click **Run Cell**. The graph should be updated with the latest values. 
20. Wait for another minute and run the cell again. Repeat this process for 5 minutes. At this point, the data in the graph should start to level off. This is because for the first four minutes, the number of orders in the database is starting from a low point (no orders). After 5 minutes, the number of orders in the preceding 5 minute window should be reasonably constant, and the rolling revenue should also tend to even out.
21. In the toolbar at the top of the notebook, click **Stop Execution**.
22. Close the **Azure Databricks** portal.
23. In Visual Studio 2017, on the **Debug** menu, click **Stop Debugging**.
24. Close **Visual Studio 2017**.

## Exercise 4: Handling product deliveries

### Task 1: Create an Azure IoT Hub

1. In the Azure portal, in the left panel, click **+ Create a Resource**.
2. On the **New** blade, in the search box, type **IoT**, and then press Enter.
3. On the **Everything** blade, click **IoT Hub**, and then click **Create**.
4. On the **IoT hub** blade, on the **Basics** tab, in the **Resource Group** drop-down list, click **20777Mod07**.
5. In the **Region** drop-down list, click the region closest to your current location.
6. In the **IoT Hub Name** box, type **20777a-iothub-&lt;your name&gt;-&lt;the day&gt;**, and then click **Size and Scale**.
7. On the **Size and Scale** tab, in the **Pricing and scale tier** drop-down list, click **B1: Basic Tier**. 
8. Leave the remaining settings at their default values, and then click **Review + create**.
9. On the **Review + create** tab, click **Create**.
10. Wait while the IoT hub is created and deployed.
11. In the Azure portal, in the left panel, click **All resources**, and then click **20777a-iothub-&lt;your name&gt;-&lt;the day&gt;**.
12. On the **20777a-iothub-&lt;your name&gt;-&lt;the day&gt;** blade, under **Settings**, click **Shared access policies**, and then click **device**.
13. Make a note of the **Connection string-primary key** value.
14. Close the **device** blade.
15. On the **20777a-iothub-&lt;your name&gt;-&lt;the day&gt;** blade, under **Explorers**, click **IoT devices**, and then click **+ Add**.
16. On the **Add Device** blade, in the **Device ID** box, type **Scanner**.
17. Leave the other settings as their default values, and then click **Save**.

### Task 2: Create an Azure Stream Analytics Job

1. In the left panel, click **+ Create a resource**.
2. On the **New** blade, in the search box, type **Stream**, and then press Enter.
3. On the **Everything** blade, click **Stream Analytics job**, and then click **Create**.
4. On the **New Stream Analytics job** blade, in the **Job name** box, type **20777a-job-&lt;your name&gt;-&lt;the day&gt;**.
5. In the **Resource group** drop-down list, click **20777Mod07**.
6. In the **Location** drop-down list, click the location closest to your current location.
7. Set **Streaming units** to **6**, and then click **Create**.
8. Wait while the stream analytics job is created.

### Task 3: Create a Cosmos DB collection for holding the details of product deliveries

1. In the Azure portal, in the left panel, click **All resources**, and then click **20777a-mod7-sql-&lt;your name&gt;-&lt;the day&gt;**.
2. On the **20777a-mod7-sql-&lt;your name&gt;-&lt;the day&gt;** blade, click **Data Explorer**, right-click **Adventure-Works**, and then click **New Collection**.
3. On the **Add Collection** blade, in the **Collection Id** box, type **Deliveries**.
4. Under **Storage Capacity**, click **Unlimited**.
5. In the **Partition key** box, type **/productnumber**, and then click **OK**.

### Task 4: Connect the IoT Hub, Stream Analytics Job, and Cosmos DB Database

1. In the left panel, click **All resources**, and then click **20777a-job-&lt;your name&gt;-&lt;the day&gt;**.
2. On the **20777a-job-&lt;your name&gt;-&lt;the day&gt;** blade, under **Job topology**, click **Inputs**, click **+ Add stream input**, and then click **IoT Hub**.
3. On the **IoT Hub** blade, in the **Input alias** box, type **ScannerDevicesHub**.
4. In the **Shared access policy name** drop-down list, click **service**.
5. Leave the remaining fields at their default values, and then click **Save**.
6. On the **20777a-job-&lt;your name&gt;-&lt;the day&gt;** blade, under **Job topology**, click **Outputs**, click **+ Add**, and then click **Cosmos DB**.
7. On the **Cosmos DB** blade, in the **Output alias** box, type **AdventureWorks**.
8. Select the **Provide Cosmos DB settings manually** check box.
9. In the **Account id** box, type **20777a-mod7-sql-&lt;your name&gt;-&lt;the day&gt;**.
10. In the **Account Key** box, enter the **PRIMARY KEY** for your **20777a-mod7-sql-&lt;your name&gt;-&lt;the day&gt;** Cosmos DB account.
11. In the **Database** box, type **Adventure-Works**.
12. In the **Collection name pattern** box, type **Deliveries**.
13. In the **Document id** box, type **id**, and then click **Save**.
14. Wait while the connection to the output is verified and validated.
15. On the **20777a-job-&lt;your name&gt;-&lt;the day&gt;** blade, under **Job topology**, click **Query**.
16. On the **Query** blade, replace the existing query with the following query, and then click **Save**.

    ```SQL
    SELECT
        id, productnumber, volumedelivered, time
    INTO
        AdventureWorks
    FROM
        ScannerDevicesHub
    ```

17. In the **Save** dialog box, click **Yes**.


### Task 5: Create the ProcessDeliveries function

> **Note:** The complete code for the **ProcessDeliveries** function is available in the file **E:\\Labfiles\\Lab07\\Solution\\Exercise04\\ProcessDeliveries.txt**.

1. In the Azure portal, in the left panel, click **All resources**, and then click **20777-func-&lt;your name&gt;-&lt;the day&gt;**.
2. On the **20777-func-&lt;your name&gt;-&lt;the day&gt;** blade, on the **Overview** tab, if the status of the app is **Stopped**, click **Start**, and wait until the **Status** is set to **Running**.
3. Click **Functions**, and then click **+ New function**.
4. On the **Choose a template below or go to the quickstart** blade, click **Cosmos DB trigger**.
5. On the **New Function** blade, in the **Name** box, type **ProcessDelivieries**.
6. Under **Azure Cosmos DB trigger**, accept the existing **Azure Cosmos DB account connection** configuration.
7. In the **Collection name** box, type **Deliveries**.
8. Select the **Create lease collection if it does not exist** check box.
9. In the **Database name** box, type **Adventure-Works**.
10. Leave **Collection name for leases** set to **leases**, and then click **Create**.
11. On the **run.csx** blade, click **20777-func-&lt;your name&gt;-&lt;the day&gt;**.
12. Under **Configured features**, click **Application settings**.
13. On the **Application settings** tab, in the **Application settings** section, click **+ Add new setting**.
14. In the **APP SETTING NAME** box, type **EndpointUrl**.
15. In the **VALUE** box, type the **URI** of your **20777a-mod7-sql-&lt;your name&gt;-&lt;the day&gt;** Cosmos DB account.
16. Click **+ Add new setting**.
17. In the **APP SETTING NAME** box, type **PrimaryKey**.
18. In the **VALUE** box, type the **primary key** of your **20777a-mod7-sql-&lt;your name&gt;-&lt;the day&gt;** Cosmos DB account.
19. Click **+ Add new setting**.
20. In the **APP SETTING NAME** box, type **Database**.
21. In the **VALUE** box, type **Adventure-Works**.
22. Click **+ Add new setting**.
23. In the **APP SETTING NAME** box, type **Collection**.
24. In the **VALUE** box, type **Data**, and then click **Save**.
25. In the left pane, click the **ProcessDeliveries** function.
26. On the **run.csx** blade, at the top of the file, under the reference to the **Microsoft.Azure.DocumentDB.Core** assembly, add a reference to the **Newtonsoft.Json** assembly, as shown by the following code:

    ```CSharp
    #r "Newtonsoft.Json"
    ```

27. Add the following **Using** directives to those already in the script:

    ```CSharp
    using System.Configuration;
    using System.Net;
    using Microsoft.Azure.Documents.Client;
    using Newtonsoft.Json;
    ```

28. After the closing brace of the **Run** function, add the following class definitions to the file:

    ```CSharp
    // Document types representing delivery records and products
    class ProductDeliveryRecord: Document
    {
        [JsonProperty("id")]
        public string ID { get; set; }

        [JsonProperty("productnumber")]
        public string ProductNumber { get; set; }

        [JsonProperty("volumedelivered")]
        public int Volume { get; set; }

        [JsonProperty("time")]
        public DateTime Time { get; set; }

        public override string ToString()
        {
            return $"Product: {ProductNumber}, Volume: {Volume}, Time: {Time}";
        }
    }

    public abstract class DocumentType: Document
    {
        [JsonProperty("doctype")]
        public string DocType { get; set; }

        public DocumentType()
        {
            this.DocType = this.GetType().Name;
        }
    }

    public class ProductCategoryData : DocumentType
    {
        [JsonProperty("partitionkey")]
        public string Subcategory { get; set; }

        [JsonProperty("category")]
        public string Category { get; set; }
    }

    public class ProductDocumentData
    {
        [JsonProperty("documenttitle")]
        public string DocumentTitle { get; set; }

        [JsonProperty("documentsummary")]
        public string DocumentSummary { get; set; }

        [JsonProperty("document")]
        public string Document { get; set; }
    }

    public class ProductImageData
    {
        [JsonProperty("diagram")]
        public string Diagram { get; set; }

        [JsonProperty("thumbnail")]
        public string Thumbnail { get; set; }

        [JsonProperty("largephoto")]
        public string LargePhoto { get; set; }
    }

    public class Product : DocumentType
    {
        [JsonProperty("id")]
        public string ProductID { get; set; }

        [JsonProperty("partitionkey")]
        public string Subcategory { get; set; }

        [JsonProperty("productcategory")]
        public string Category { get; set; }

        [JsonProperty("productname")]
        public string ProductName { get; set; }

        [JsonProperty("productnumber")]
        public string ProductNumber { get; set; }

        [JsonProperty("color")]
        public string Color { get; set; }

        [JsonProperty("listprice")]
        public decimal ListPrice { get; set; }

        [JsonProperty("size")]
        public string Size { get; set; }

        [JsonProperty("weight")]
        public string Weight { get; set; }

        [JsonProperty("quantityinstock")]
        public int QuantityInStock { get; set; }

        [JsonProperty("model")]
        public string Model { get; set; }

        [JsonProperty("description")]
        public string Description { get; set; }

        [JsonProperty("documentation")]
        public ProductDocumentData Documentation { get; set; }

        [JsonProperty("images")]
        public ProductImageData Images { get; set; }
    }
    ```

29. Modify the definition of the **Run** method to indicate that it supports asynchronous operations.

    ```CSharp
    public static async Task Run(IReadOnlyList<Document> input, ILogger log)
    ```

30. Remove the following two statements from the body of the function:

    ```CSharp
    log.LogInformation("Documents modified " + input.Count);
    log.LogInformation("First document Id " + input[0].Id);
    ```

31. Above the **if** statement, add the following lines of code that retrieve the configuration settings required to connect to the Data collection in the **Adventure-Works** database, and then use these settings to create a DocumentClient object:

    ```CSharp
    string endpointUrl = ConfigurationManager.AppSettings["EndpointUrl"];
    string primaryKey = ConfigurationManager.AppSettings["PrimaryKey"];
    string database = ConfigurationManager.AppSettings["Database"];
    string collection = ConfigurationManager.AppSettings["Collection"];

    var client = new DocumentClient(new Uri(endpointUrl), primaryKey);
    ```

32. Inside the **if** statement, add the following statements:

    ```CSharp
    // Iterate through the change feed
    foreach (Document doc in input)
    {

    }
    ```

33. Inside the **foreach** block, add the following code:

    ```CSharp
    // Deserialize each document - it should be a ProductDeliveryRecord
    ProductDeliveryRecord deliveryDoc = JsonConvert.DeserializeObject<ProductDeliveryRecord>(doc.ToString());

    // Display the details on the trace log
    log.LogInformation(deliveryDoc.ToString());
    ```
34. Inside the **foreach** block, immediately after the code you have just entered, add the following statements:

    ```CSharp
    // Update the volume in stock in the Data collection in the Adventure-Works database
    var dataCollectionUri = UriFactory.CreateDocumentCollectionUri(database, collection);
    var productQueryText = "SELECT * FROM c WHERE c.doctype = 'Product' AND c.productnumber = @productnum";
    Product productDoc;
    ```

35. Add the following code after the statements that you entered in the previous step. This code runs the query that fetches the corresponding product from the **Data** collection, and then updates the **QuantityInStock** field with the value from the new delivery. The **SaveDocument** function attempts to write the updated product document back to the **Data** collection. The **SaveDocument** returns a boolean that indicates whether the save operation was successful; while customers are placing orders, the stock levels are updated, and it is important not to lose these changes. The **SaveDocument** method uses ETags to prevent this situation from occuring. The *do* loop iterates, repeatedly fetching the latest version of the product document and attempting to save it until the operation is successful.

    ```CSharp
    do
    {
        // Find the product in the Data collection
        var productParam = new SqlParameterCollection { 
            new SqlParameter("@productnum", deliveryDoc.ProductNumber)
        };

        var productQuery = client.CreateDocumentQuery(dataCollectionUri, new SqlQuerySpec
        {
            QueryText = productQueryText,
            Parameters = productParam
        }, new FeedOptions
        {
            EnableCrossPartitionQuery = true
        }).AsEnumerable();

        productDoc = productQuery.Take(1).Single();
        log.LogInformation($"product is {productDoc.ProductName}"); 

        // Update the quantityinstock field of the product doc to reflect the new delivery
        productDoc.QuantityInStock += deliveryDoc.Volume;
    }
    // Save the updated product doc back to the database.
    // If the Save operation fails, it could be due to a conflict caused by a simultaneous update such as a customer placing an order.
    // If so, try querying the product to get the most up to date information and save it again
    while(! await SaveDocument(productDoc, client, log));
    ```

36. After the closing brace of the **Run** method, but above the definition of the **ProductDeliveryRecord** class, add the following function. This function saves the document, using the ETag to ensure that the document has not been modified by another user since it was retrieved.

    ```CSharp
    // Save a document and check for conflicts by using the ETag
    // Return true if the update was successful, false otherwise
    private static async Task<bool> SaveDocument(Document doc, DocumentClient client, ILogger log)
    {
        try
        {
            // Replace the document in the database with the updated doc
            RequestOptions options = new RequestOptions
            {
                AccessCondition = new AccessCondition
                {
                    Condition = doc.ETag,
                    Type = AccessConditionType.IfMatch
                }
            };

            var response = await client.ReplaceDocumentAsync(doc.SelfLink, doc, options);
            log.LogInformation($"Document replaced. Status code is {response.StatusCode}");
            return true;
        }
        catch (DocumentClientException dce)
        {
            // If a conflict occured, display a message
            if (dce.StatusCode == HttpStatusCode.PreconditionFailed)
            {
                log.LogInformation("Update failed - another user already changed this document. Requery and try again");
            }
            return false;
        }
    }
    ```

37. Click **Save**.

### Task 6: Simulate deliveries arriving at the warehouse

1. In the left panel, click **All resources**, and then click **20777a-job-&lt;your name&gt;-&lt;the day&gt;**.
2. On the **20777a-job-&lt;your name&gt;-&lt;the day&gt;** blade, click **Overview**, and then click **Start**.
3. On the **Start job** blade, click **Start**. Wait for the stream analytics job to start before continuing.
4. On the Windows Start menu, click **Visual Studio 2017**.
5. On the **File** menu, point to **Open**, and then click **Project/Solution**.
6. In the **Open Project** dialog box, go to **E:\\Labfiles\\Lab07\\Starter\\Exercise04\\ProductScanner**, and then double-click **ProductScanner.sln**.
7. In Solution Explorer, click **App.config**.
8. On the **App.config** tab, in the **appSettings** section, replace **\~CONNECTION STRING\~** with the **Connection string-primary key** values that you noted earlier for the **20777a-iothub-&lt;your name&gt;-&lt;the day&gt;**.
9. Press F5 to build and run the application. You should see messages appearing at 5 second intervals representing new deliveries.
10. Return to the Azure portal, click **All resources**, and then click **20777a-mod7-sql-&lt;your name&gt;-&lt;the day&gt;**, and then click **Data Explorer**.
11. In the **SQL API** pane, expand **Adventure-Works**, expand **Deliveries**, and then click **Documents**. Verify that some delivery documents have been created. These documents have been added by the Azure Stream Analytics job, using the data streamed from the IoT Hub.
12. Click any document and examine its contents. It should look similar to this:

    ```JSON
    {
       "id": "24037fa0-06b9-404a-912d-017464462f63",
        "productnumber": "TO-2301",
        "volumedelivered": 99,
        "time": "2018-08-22T12:28:07.2210179Z",
        "_rid": "T3ILALIG40ATAAAAAAAAAA==",
        "_self": "dbs/T3ILAA==/colls/T3ILALIG40A=/docs/T3ILALIG40ATAAAAAAAAAA==/",
        "_etag": "\"01008a9e-0000-0000-0000-5b7d56dc0000\"",
        "_attachments": "attachments/",
        "_ts": 1534940892
    }
    ```

13. In the left panel, click **All resources**, and then click **20777a-func-&lt;your name&gt;-&lt;the day&gt;**.
14. Click the **ProcessDeliveries** function, and then click **Logs**. You should see log messages (similar to those below) appearing as the documents are added to the **Deliveries** collection and are processed by the function.

    ```Text
    2018-08-22T17:50:42.663 [Info] Function started (Id=7330d281-2108-4c89-a262-7db94d20bcf3)
    2018-08-22T17:50:42.663 [Info] Product: TT-T092, Volume: 98, Time: 8/22/2018 5:50:34 PM
    2018-08-22T17:50:44.216 [Info] product is Touring Tire Tube
    2018-08-22T17:50:44.452 [Info] Document replaced. Status code is OK
    2018-08-22T17:50:44.452 [Info] Function completed (Success, Id=7330d281-2108-4c89-a262-7db94d20bcf3, Duration=1785ms)
    2018-08-22T17:50:49.793 [Info] Function started (Id=f5980a1f-6f9b-41a5-b5e1-78dc05a329f5)
    2018-08-22T17:50:49.793 [Info] Product: LI-5800, Volume: 86, Time: 8/22/2018 5:50:39 PM
    2018-08-22T17:50:51.592 [Info] product is Internal Lock Washer 10
    2018-08-22T17:50:51.811 [Info] Document replaced. Status code is OK
    2018-08-22T17:50:51.811 [Info] Function completed (Success, Id=f5980a1f-6f9b-41a5-b5e1-78dc05a329f5, Duration=2027ms)
    ```

### Task 7: Cleanup the lab environment

1. In Visual Studio 2017, on the **Debug** menu, click **Stop Debugging**.
2. Close Visual Studio.
3. In Internet Explorer, in the left panel, click **Resource groups**.
4. On the **Resource groups** blade, right-click  **20777Mod07**, and then click **Delete resource group**.
5. On the **Are you sure you want to delete "20777Mod07"?** blade, in the **TYPE THE RESOURCE GROUP NAME** box, type **20777Mod07**, and then click **Delete**.

---
 2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
